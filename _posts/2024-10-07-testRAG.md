---
layout: post
title: Image construction experiment
date: 2024-08-30 16:15:09
description: 
tags: formatting images
categories: encoding method
tabs: true
related_posts: false
toc: 
  sidebar: left
---


Text generation is easy in that it's constructed based on a series of discrete tokens, which is easy to  construct. Image generation posed greater challenge due to the high dimension of image space. Each pixel is three dimension, corresponding to RGB. for an image with millions of pixels, it'll be too computatianl expensive if we generate pixels by pixels. Moreover, each pixel is depended not only on previous pixels but also every adjacent pixels. A more appropriate approach is to consider each image as a sampling on a latent distribution over image. By using this approach, the model can produce images faster than producing each pixels. Since this blog will discuss various ways of generating image, detailed deduction of this algorithm will be referenced by other posts. 

# VAE

VAE is a type of image construction algorithm, including encoder and decoder. Encoder receive input image and produce mean vectors and deviation vectors that define a series of independent normal distributions. Then the model will draw a sample from the distributions and use decoder to produce an image.

The number of dimension in mean and deviation vectors has a significant impact on the quality of image generation. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_4_2.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_4_4.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_4_10.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    From left to right, the dimension in mean vector and deviation vectors is 2, 4, 10
</div>

As the latent space dim increased, the quality of produced image significantly improves. 

However, when we added more details in the image, such as CIFAR10, the quality of generated image drastically decreases.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_cifar10.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_cifar101.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_cifar102.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    I try to improve performance of VAE on CIFAR10 by only training on 200, 100, 25 images. However, the generated images are very blurry.
</div>

As I changed the size of train dataset, there's an interesting phenomenon regarding image generation.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_cifar103.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_cifar105.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/image construction/VAE_cifar104.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    The model is trained on a single batch of 2 images. The left image is the generated image.
</div>

We can see that the generated image is like taking the average of the images in a batch. This is probably 
because when doing back propagation, the model is doing propagation on all images and takes the average of the gradient. Thus, the final image will resemble all the images in the batch.

