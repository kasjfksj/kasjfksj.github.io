---
layout: post
title: Diffusion model
date: 2024-11-08 16:15:09
description: 
tags: formatting images
categories: model architecture
tabs: true
related_posts: false
toc:  
  sidebar: left
---

Reasoning and perception, two of the most important functionality in human being, are something I'm always pondering about. How could I use current AI architecture to tackle these tasks. 

For reasoning, we have the llm to process discretized text or tokens. For perception, which contains infinite states that a single word can't describe, should be diffusion model that can model any given distributions, at least from my opinion. It can model either image distribution, or image distribution conditioned on certain text, or even distribution of action from diffusion policy. However, no matter what distribution it aims to produce, it always starts from a simple Guassian distribution. 

My question arises as to whether diffusion model can start from a prior distribution that's already conditioned on some information. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/4.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Yes, there's guidance model, classifier or classifier-free that can guide unconditional model towards our desired distribution using gradient of data distribution. The mathematical deduction of such guidance method is very beautiful and incontroversial. However, when I look at the model architecture, the model seems clumsy with cross attention with text. If I manage to diffuse model from prior distribution without use of guidance, then it saves the computation of the cross attention.

Therefore, the following blog aims to answer that question.



# Diffusion

Back to diffusion, as I delve more into the model, I realize that it's more than just transform noise back to original image. Rather, it's about transforming distribution to another distribution via a certain trajectory, which is analogous to flow-matching. The only different between the two is that diffusion introduces random noise where as flow-matching transform in a more deterministic way. 

For the next part, I'll explain how diffusion model works from a mathematical view point, which involves too much on measure theory which I have just begun to learn, but I'll try my best to explain.

Diffusion model can be seen as learning the predefined trajectory between 2 know distribution in a 2-Wasserstein Space. That trajectory is defined by diffusion process. What the model learns is the tangential space of that trajectory, or conditional probability. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/5.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>



Unsatisfied by the assumption that every image must derive from Gaussian distribution. I hope that image comes from a prior distribution, something that's determined by the language, and the images that contain that semantic information is a random sample around that information. Therefore, I modify the prior Gaussian distribution such that it comes from a conditional. 



For that, I copy the code from  and modify the noise term. 


However, when I try to produce a series of images of 5, I got the following problem.  


A distribution surrounding a specific Gaussian distribution doesn't map to image distribution of 5. Instead, it matches to other distributions of images.

My theory as to why it deviates is the following:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

While I hope that original distribution , but in reality, the trajectory to different kinds of images can intersect with each other, making the model difficult to determine which trajectory should the model takes. 

While in original diffusion that diffuses from a Gaussian distribution, it starts from a single point on the space, ensuring that when it diffuses to other distribution of images, it doesn't intersect with each other. 

Another limitations when I consider this approach is that different semantic distribution will have different variations depending on the abstraction of the text. A word "Thing" will surely has massive variations to cover all possible subjects. A word "dog", a subset of the word "Thing", will surely have less variations in prior distributions. Modeling variances based on the abstraction of word is really difficult, at least from my knowledge.

Although mathematically, it's 

