---
layout: post
title: Diffusion model
date: 2024-11-08 16:15:09
description: 
tags: formatting images
categories: model architecture
tabs: true
related_posts: false
toc:  
  sidebar: left
---

Reasoning and perception, two of the most important functionality in human being, are something I'm always pondering about. How could I use current AI architecture to tackle these tasks. 

For reasoning, we have the llm to process discretized text or tokens. For perception, which contains infinite states that a single word can't describe, should be diffusion model that can model any given distributions, at least from my opinion. It can model either image distribution, or image distribution conditioned on certain text, or even distribution of action which comes from diffusion policy. However, no matter what distribution it aims to produce, it always starts from a simple Guassian distribution. 

My question arises as to whether diffusion model can start from a prior distribution that's already conditioned on some information. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/4.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Yes, there's guidance model, classifier or classifier-free that can guide unconditional model towards our desired distribution using gradient of data distribution. The mathematical deduction of such guidance method is very beautiful and incontroversial. However, when I look at the model architecture, the model seems clumsy with cross attention with text. If I manage to diffuse model from prior distribution without use of guidance, then it saves the computation of the cross attention.

Therefore, the following blog aims to answer that question of whether the diffusion model can be conditioned on text without passing semantic information at every timestep.

# Diffusion

Back to diffusion, as I delve more into the model, I realize that it's more than just transform noise back to original image. Rather, it's about transforming distribution to another distribution via a certain trajectory, which is analogous to flow-matching. The only different between the two is that diffusion introduces random noise where as flow-matching transform in a more deterministic way. 

For the next part, I'll explain how diffusion model works from a mathematical view point, which involves too much on measure theory which I have just begun to learn, but I'll try my best to explain.

Diffusion model can be seen as learning to follow the predefined trajectory between 2 known distributions in a 2-Wasserstein Space. That trajectory is defined by diffusion process. What the model learns is the tangential space of that trajectory, or conditional probability. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/5.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Traditional conditional diffusion model looks more like this where every targeted distributions starts from a Gaussian distribution.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/5.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

However, I want the diffusion model looks more like the following. Where diffusion process starts from different distributions conditioned on text.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Therefore, I need to define a diffusion trajectory such that it ensures it has targeted distribution and source distribution at their ends.

# Deduction of conditional diffusion


 
For that, I copy the code from  and modify the noise term. 

However, when I try to produce a series of images of 5, I got the following problem.  
 
A distribution surrounding a specific Gaussian distribution doesn't map to image distribution of 5. Instead, it matches to other distributions of images.

My theory as to why it deviates is the following:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AGI/3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

While I hope that original distribution , but in reality, the trajectory to different kinds of images can intersect with each other, making the model difficult to determine which trajectory should the model takes. 

While in original diffusion that diffuses from a Gaussian distribution, it starts from a single point on the space, ensuring that when it diffuses to other distribution of images, it doesn't intersect with each other. 

Of course, I still can't test whether the hypothesis is true due to unfeasibility of plot how data migrates in high dimension. I consider to experiment it on data in low dimension, but I'm unsure whether experiment in low dimension can shed light over high dimension due to 

Another limitations when I consider this approach is that different semantic distribution will have different variations depending on the abstraction of the text. A word "Thing" will surely has massive variations to cover all possible subjects. A word "dog", a subset of the word "Thing", will surely have less variations in prior distributions. Modeling variances based on the abstraction of word is really difficult, at least from my knowledge.



