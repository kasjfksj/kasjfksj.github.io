---
layout: post
title: Improving ZOD-MC
date: 2025-12-22 16:15:09
description: 
tags: formatting images
categories: model architecture
tabs: true
related_posts: false
toc: 
  sidebar: left
---

# Background
At the beginning of the quarter, advised by my professor, I began to dig deep into this paper: Zeroth-Order Diffusion Monte Carlo (ZOD-MC). The idea is fascinating—a way to sample from tricky distributions without needing them to be log-concave or satisfy fancy inequalities like Log-Sobolev. Also, it uses only zeroth-order queries to the potential V, no gradients. It employs denoising diffusion to handle multimodality and even discontinuities, which is cool for real-world stuff like non-convex potentials with high barriers. I feel like this can be a great starting point for studying and researching sampling and diffusion processes. And I started my journey from here.

# Thought Process

But as I read through, the curse of dimensionality hit me hard. The oracle complexity is exp(Õ(d) log(1/ε)), great for low d but very inefficient in high dimensions. In fact, many of the sampling methods it references are either exponential or pseudo-exponential for non-concave functions, or polynomial but concave or with other nice properties. This got me thinking: how to make it better to improve on zero-order sampling? My professor suggested starting with simple cases, like when dimensions are independent or the potential decomposes into lower-dim parts.

That sparked the "partial ZOD-MC" idea. If V(x) breaks into sum V_j(x_j) over b blocks, each of size d/b, I could run ZOD-MC on each block separately. Parallelize it, concatenate samples. Complexity drops to O(b) * exp(Õ(d/b) log(1/ε)). Feels like a win for independent distributions – tested mentally on separable mixtures, seems efficient.

However, implementing the code is quite hard work since the code in the paper only deals with full dimension (log_prob). And no distributions are provided to construct an independent distribution. I need to write log_prob for single dimension and then write custom independent distribution to test my sampling method. After months of efforts, I finally got my method working, and it’s efficient and works perfectly on independent distributions. Which is, although not so significant since it only works on independent distributions, a solid step.

Next, I also tried brainstorming other optimizations, like tweaking the rejection sampling proposal. Maybe a better envelope than Gaussian to boost acceptance. But it doesn't fit at first. Then, I looked more closely into the paper and found out that the whole thing relies on the OU process and Tweedie's formula, where the conditional is V plus a quadratic – which is why Gaussian is useful and can’t be replaced. Changing it would mess up the score estimation and guarantees from Theorem 1.