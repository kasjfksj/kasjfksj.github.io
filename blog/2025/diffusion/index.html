<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Diffusion Series - DDPM model | Xiaoyang Li </title> <meta name="author" content="Xiaoyang Li"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kasjfksj.github.io/blog/2025/diffusion/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Xiaoyang</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Diffusion Series - DDPM model</h1> <p class="post-meta"> Created in August 19, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/formatting"> <i class="fa-solid fa-hashtag fa-sm"></i> formatting</a>   <a href="/blog/tag/images"> <i class="fa-solid fa-hashtag fa-sm"></i> images</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>The purpose writing this blog is to document what I’ve learnt so far about diffusion. To me diffusion is a powerful paradigm and a very interested research area where math (especially probability) collide with AI most. So I want to document everything I’ve learnt so far so that readers can gain better understanding of diffusion model.</p> <p>Since there are so many stuff about diffusion, I will write several blogs on it.</p> <p>The beginning of diffusion model is for image generation. Thus, before discussing diffusion model. First discuss the task of image generation and the probability behind it</p> <h1 id="probability-view-of-image-generation">Probability view of Image generation</h1> <p>The task of image generation is simple: generating realistic images that align with the style of training data, but what does it even mean to “generate realistic images”?</p> <p>Having a strict standard or formula to dictate the realness of an image is not feasible as there are numerous pixels to be considered and the big number of pictures to consider. Therefore, it’s much more appropriate to consider form a probability point of view where we basically want to sample from an unknown probability distribution of real images, denoted as \(p_{data}(x)\).</p> <p>\(x\) with high \(p_{data}(x)\) look like real photographs, x with tiny (near-zero) \(p_{data}(x)\) look like random color patches or just noise.</p> <p>However, such distribution is unknown. Therefore it’s best to have a model that can model data distribution with its own distribution of \(p_{\theta}(x)\) where we want such distribution is as close to \(p_{data}(x)\) as possible.</p> <p>And the objective should be \(\underset{\theta}{argmin} \,\mathcal{F}(p_{data}(x),\,p_{\theta}(x))\) where \(\mathcal{F}\) is the measurement of the distance of two distributions.</p> <p>Of course, the traditional way of measuring distance of 2 distributions is KL divergence, which is given as follow:</p> \[D_{\text{KL}}\!\bigl(p_{\text{data}} \,\ \mid \, p_\theta\bigr) = \mathbb{E}_{x \sim p_{\text{data}}(x)} \!\Bigl[\log p_{\text{data}}(x) - \log p_\theta(x)\Bigr]\] <p><strong>Note:</strong> \(p_{\text{data}}(x)\) is fixed (it’s the real world), so minimizing this KL is <strong>exactly</strong> equivalent to maximizing:</p> \[\mathbb{E}_{x \sim p_{\text{data}}(x)} \!\bigl[\log p_\theta(x)\bigr]\] <h2 id="vae">VAE</h2> <p>However, learning such \(p_{\theta}(x)\) is not so easy since \(x\) typically lies in a very high-dimensional space — for example, a 256×256 color image has dimension 196,608. Modeling the distribution on such high dimension typically causes greater computation and curse of dimension (volume grows exponentially with dimension, requiring exponential number of data to accurately estimate the true distribution of the data).</p> <p>Thus, we want to introduce a model where it generates samples on low-dimensional latent space \(z\), 128 dimension or 512 dimension. We denote such probabilistic model as \(p_\theta(x \mid z) \;=\; \mathcal{N}\big(x \mid \mu_\theta(z),\; \sigma^2 I\big)\) where \(\mu_θ(z)\) is the image output by the decoder network.</p> <p>Also we want the latent space have certain prior, usually \(\mathcal{N}(0,I)\), so that we can easily sampled \(z\). The observed data is then modeled as: \(p_\theta(x) = \int p_\theta(x \mid z) \, p(z) \, dz\)</p> <p>where \(p_\theta(x \mid z)\) is a flexible decoder (typically a deep neural network)</p> <p>You might wonder at this point, why don’t we directly train a model via monte carlo estimation where \(p_\theta(x) \approx \frac{1}{S} \sum_{s=1}^S p_\theta(x \mid z_s),\quad z_s \sim p(z)\)</p> <p>This would work mathematically. But in reality, it performs very bad, because the prior is still in high-dimension, so most sampled \(z_s \sim \mathcal{N}(0,I)\) have tiny (almost zero) likelihood \(p_\theta(x \mid z_s)\)</p> <p>Occasionally, by pure chance, you draw a \(z_s\) that is close to the region where the decoder puts mass on \(x\); then \(p_\theta(x \mid z_s)\) is huge. This is why training model via Monte Carlo estimate from purely Gaussian distribution has insane variance and fails completely.</p> <p>But during training we are not blind — we have the actual data point x as prior. So instead of randomly sample from the prior, we can directly sample \(z\) based on original image \(x\). This can be modeled as conditional distribution \(q_{\phi}(z\mid x)\) which is essentially mapping image distribution to the latent distribution. Such conditional distribution is also parameterized by an encoder network which produces the mean and the variance: \(q_\phi(z \mid x) = \mathcal{N}\bigl(z \;\big|\; \mu_\phi(x),\,\operatorname{diag}(\sigma_\phi^2(x))\bigr)\)</p> <p>With this in mind, we can now rewrite \(\log p_{\theta}(x)\) as the following</p> \[\begin{aligned} \log p_\theta(x) &amp;= \int q_\phi(z \mid x) \log p_\theta(x) \, dz \\ &amp;= \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{p_\theta(z \mid x)} \, dz \\ &amp;= \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \, dz + \int q_\phi(z \mid x) \log \frac{q_\phi(z \mid x)}{p_\theta(z \mid x)} \, dz \\ &amp;= \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \, dz + D_{\text{KL}}(q_\phi(z \mid x) \ \mid p_\theta(z \mid x)) \\ &amp; \geq \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \, dz \quad \text{(since KL} \geq 0\text{)} \end{aligned}\] <p>Now rewite this integral form:</p> \[\int q_\phi(z \mid x) \log \frac{p_{\theta}(x \mid z) p(z)}{q_\phi(z \mid x)} dz = \int q_\phi(z \mid x) \log p_{\theta}(x \mid z) dz + \int q_\phi(z \mid x) \log \frac{p_{\theta}(z)}{q_\phi(z \mid x)} dz\] \[= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] - D_{\text{KL}}(q_\phi(z \mid x) \ \mid p_{\theta}(z))\] <p>summing all up:</p> \[\log p_{\theta}(x) \ge \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] - D_{\text{KL}}(q_\phi(z \mid x) \ \mid p_{\theta}(z))\] <p>Remember that our objective is to maximizing</p> \[\mathbb{E}_{x \sim p_{\text{data}}(x)} \!\bigl[\log p_\theta(x)\bigr]\] <p>So the training objective wil become minimizing:</p> \[-\mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] + D_{\text{KL}}(q_\phi(z \mid x) \ \mid p(z)) \quad x \sim p_{data}(x)\] <p>The first term pushes the decoder to reconstruct \(x\) well from encoder samples.</p> <p>The second term forces the encoder to stay close to the standard Gaussian prior.</p> <p>In practice we take one sample \(z \sim q_\phi(z \mid x)\) (reparameterised) for the expectation and compute the KL analytically. (For more details check online description of VAE architecture and the loss)</p> <h2 id="ddpm">DDPM</h2> <p>So far, Variational Autoencoders seems to work well on learning compressed representations of data. They encode an image into a small latent vector and decode it back, while trying to keep the latent codes close to a simple Gaussian distribution \(\mathcal{N}(\mathbf{0}, \mathbf{I})\).</p> <p>The problem is that real-world data (like photos of faces or animals) has a very complicated distribution. Forcing everything through a small bottleneck and a simple Gaussian prior makes it hard for VAEs to capture all the sharp details. As a result, images generated by VAEs often look blurry.</p> <p>So, can we model complex data distributions more accurately?</p> <p>The answer is yes. Instead of trying to jump from simple Gaussian noise to complex data in one huge step (like VAEs do with decoding), we build a smooth probabilistic trajectory connecting the data distribution to a simple Gaussian noise. Each point on this trajectory is a slightly noisier version of the data. By breaking the entire path into many small steps, the change between consecutive points becomes tiny and easy to model. The neural network only needs to learn these small perturbations—predicting and removing a little noise at each step—which is a much simpler task than reconstructing the full image all at once.</p> <p>In practice, we can define a forward process that iteratively adds predetermined Gaussian noise to an image and then try to learn a backward process to convert Gaussian noise to image.</p> <p>The concept is intuitive, but the math that makes it work efficiently is clever and complicated.</p> <h3 id="forward-process-gradually-adding-noise">Forward Process: Gradually Adding Noise</h3> <p>We start with a clean image \(\mathbf{x}_0\). At each timestep \(t = 1\) to \(T\) (usually \(T \approx 1000\)), we add a small independent Gaussian noise:</p> \[\mathbf{x}_t = \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1} + \sqrt{\beta_t} \, \boldsymbol{\epsilon_t}, \quad \boldsymbol{\epsilon_t} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\] <p>Here \(\beta_t\) is a small variance schedule (\(\beta_t \to 1\) as \(t\) increases).</p> <p>To make future mathematical deduction easier, let \(\alpha_t = 1 - \beta_t\), the formula becomes:</p> \[\mathbf{x}_t = \sqrt{\alpha_t} \, \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t}\] <p>Basically, we define the forward process to be: \(p(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \sim \mathcal{N}(\sqrt{\alpha_t} \, \mathbf{x}_{t-1}, 1-\alpha_t)\)</p> <p>Before retrieving backward process \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\), we need to make a mathematical transformation:</p> \[\begin{aligned} \mathbf{x}_t &amp;= \sqrt{\alpha_t} \, \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t} \\ &amp;= \sqrt{\alpha_t} \, (\sqrt{\alpha_{t-1}} \, \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \, \boldsymbol{\epsilon_{t-1}}) + \sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t} \\ &amp;= \sqrt{\alpha_t} \, \sqrt{\alpha_{t-1}} \, \mathbf{x}_{t-2} + \sqrt{\alpha_t} \,\sqrt{1-\alpha_{t-1}} \, \boldsymbol{\epsilon_{t-1}}+\sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t} \end{aligned}\] <p>We can treat \(\sqrt{\alpha_t} \,\sqrt{1-\alpha_{t-1}} \, \boldsymbol{\epsilon_{t-1}}\) and \(\sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t}\) as representation of Gaussian distributions:</p> \[\begin{aligned} \mathbf{X}_1 &amp;\sim \sqrt{\alpha_t}\,\sqrt{1-\alpha_{t-1}}\,\boldsymbol{\epsilon}_{t-1} = \mathcal{N}\!\left(0,\, \alpha_t(1-\alpha_t)\right) \\ \mathbf{X}_2 &amp;\sim \sqrt{1-\alpha_t}\,\boldsymbol{\epsilon}_t = \mathcal{N}\!\left(0,\, (1-\alpha_t)\right) \end{aligned}\] <p>Since the noise are independently added, \(\mathbf X_1 + \mathbf X_2 \sim \mathcal{N}(0, \, 1-\alpha_t \, \alpha_{t-1})\), so the formula becomes: \(\mathbf{x}_t = \sqrt{\alpha_t \, \alpha_{t-1}} \, \mathbf{x}_{t-2} + \sqrt{1-\alpha_t \, \alpha_{t-1}} \, \epsilon\)</p> <p>Applying this procedure recursively, we’ll get:</p> \[\mathbf{x}_t = \sqrt{\alpha_t \, \alpha_{t-1}\,...\, \alpha_1} \, \mathbf{x}_0 + \sqrt{1-\alpha_t \, \alpha_{t-1} \, ... \, \alpha_1} \, \epsilon\] <p>Let \(\bar{\alpha_t} = \alpha_t \, \alpha_{t-1}\,...\, \alpha_1\), we can simplify the above fomula to be \(\mathbf{x}_t = \sqrt{\bar{\alpha_t}} \, \mathbf{x}_0 + \sqrt{1-\bar{\alpha_t}}\, \epsilon\)</p> <p>The forward process becomes \(p(\mathbf{x}_t \mid \mathbf{x}_0) \sim \mathcal{N}(\sqrt{\bar{\alpha_t}} \, \mathbf{x}_0, 1-\bar{\alpha_t})\)</p> <h3 id="backward-process">Backward Process</h3> <p>In order to get reverse process \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\), we can try to apply bayesian rule:</p> \[p(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \frac{p(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \, p(\mathbf{x}_{t-1})}{p(\mathbf{x}_t)}\] <p>However, the problem is that \(p(\mathbf{x}_t)\) and \(p(\mathbf{x}_{t-1})\) have no explicit formula, so the alternative way is to include the source image \(\mathbf{x}_0\), which rewrites the formula as the following:</p> \[\begin{aligned} p(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) &amp;= \frac{p(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) \, p(\mathbf{x}_{t-1} \mid \mathbf{x}_0)}{p(\mathbf{x}_t \mid \mathbf{x}_0)} \\ &amp;= \exp\left(-\frac{1}{2}\left(\frac{(\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x}_{t-1})^2}{1- \alpha_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\alpha_{t-1}}\mathbf{x}_0)^2}{1 - \bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t}\mathbf{x}_0)^2}{1 - \bar{\alpha}_t}\right)\right) \\ &amp;= \exp\left(-\frac{1}{2}\left(\left(\frac{\alpha_t}{1- \alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right)\mathbf{x}_{t-1}^2 - \left(\frac{2\sqrt{\alpha_t}}{1- \alpha_t}\mathbf{x}_t + \frac{2\sqrt{\alpha_{t-1}}}{1 - \bar{\alpha}_{t-1}}\mathbf{x}_0\right)\mathbf{x}_{t-1} + C(\mathbf{x}_t, \mathbf{x}_0)\right)\right) \end{aligned}\] <p>This seems similar to Gaussian distribution formula. Therefore, we ignore the last term \(C(\mathbf{x}_t, \mathbf{x}_0)\) since \(\mathbf{x}_t, \mathbf{x}_0\) are not relevent to \(\mathbf{x}_{t-1}\).</p> <p>The variance and mean of such probability function is this</p> \[\tilde{\sigma}_t^2 = 1/\left(\frac{\alpha_t}{1- \alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right) = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \alpha_t \\ \tilde{\mu}_t = \left(\frac{\sqrt{\alpha_t}}{1-\alpha_t}\mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_t}}{1 - \bar{\alpha}_t}\mathbf{x}_0\right)/\left(\frac{\alpha_t}{1-\alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)}{1 - \bar{\alpha}_t}\mathbf{x}_0\] <p>Remember that \(\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\epsilon_t\right)\), so we can rewrite the mean to be \(\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\epsilon_t\right)\)</p> <p>Therefore, \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)=\mathcal{N}(\, \tilde{\mu}_t \, , \tilde{\sigma}_t^2)\)</p> <p>However, the distribution \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)\) is not what we want since this implies that we already gain access to generated image \(\mathbf{x}_0\). The ideal distribution that we wish the model to learn is \(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\)</p> <p>This is the backward process and we want the model to learn such distribution.</p> <h3 id="elbo">ELBO</h3> <p>If you remember, for an image generation task, we want to maximize the likelihood:</p> \[\mathbb{E}_{x \sim p_{\text{data}}(x)} \!\bigl[\log q_\theta(x_0)\bigr]\] <p>However, directly maximizing this is intractable. Instead, we derive a tractable lower bound using Jensen’s inequality and properties of the diffusion process.</p> \[\begin{align*} \log q_\theta(\boldsymbol{x_0}) &amp;= \log \int q_\theta(\boldsymbol{x}_{0:T}) d\boldsymbol{x}_{1:T} \\ &amp;= \log \int \frac{q_\theta(\boldsymbol{x}_{0:T}) p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)}{p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)} d\boldsymbol{x}_{1:T} \\ &amp;= \log \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \frac{q_\theta(\boldsymbol{x}_{0:T})}{p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)} \right] \\ &amp;\geq \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_{0:T})}{p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) \prod_{t=1}^T q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{\prod_{t=1}^T p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1})} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \prod_{t=2}^T q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0) \prod_{t=2}^T p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1})} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \prod_{t=2}^T q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0) \prod_{t=2}^T p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{\frac{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0) p(\boldsymbol{x}_t|\boldsymbol{x}_0)}{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_0)}} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{\frac{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0) \cancel{p(\boldsymbol{x}_t|\boldsymbol{x}_0)}}{\cancel{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_0)}}} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{\cancel{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)}} + \log \frac{\cancel{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)}}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} + \sum_{t=2}^T \log \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \right] + \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T)}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} \right] + \sum_{t=2}^T \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_1|\boldsymbol{x}_0)} \left[ \log q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \right] + \mathbb{E}_{p(\boldsymbol{x}_T|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T)}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} \right] + \sum_{t=2}^T \mathbb{E}_{p(\boldsymbol{x}_t, \boldsymbol{x}_{t-1}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \underbrace{\mathbb{E}_{p(\boldsymbol{x}_1|\boldsymbol{x}_0)} \left[ \log q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \right]}_{\text{reconstruction term}} - \underbrace{D_{\text{KL}}(p(\boldsymbol{x}_T \mid \boldsymbol{x}_0) \| q_\theta(\boldsymbol{x}_T))}_{\text{prior matching term}} - \sum_{t=2}^T \underbrace{\mathbb{E}_{p(\boldsymbol{x}_t|\boldsymbol{x}_0)} \left[ D_{\text{KL}}(p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0) \| q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)) \right]}_{\text{denoising matching term}} \end{align*}\] <p>(This derivation is adapted from <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" rel="external nofollow noopener" target="_blank">Calvin Luo’s excellent diffusion tutorial</a> with minor notation adjustments for consistency with later sections.)</p> <p>In practice, we rarely optimize the full ELBO directly. Instead, most modern diffusion models focus on the following terms:</p> <ul> <li>Reconstruction term → usually approximated via simple noise prediction</li> <li>Denoising matching terms → the main training objective</li> </ul> <p>The prior matching term is typically very small when T is large, so it’s often ignored during training. After some algebra ,the denoising objective for each timestep boils down to this very useful form</p> \[\underset{\theta}{\arg\min} \, D_{\text{KL}}(p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0) \| q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)) = \underset{\theta}{\arg\min} \frac{1}{2\sigma_q^2(t)} \frac{\bar{\alpha}_{t-1}(1-\alpha_t)^2}{(1-\bar{\alpha}_t)^2} \left[ \left\| \hat{\boldsymbol{x}}_\theta(\boldsymbol{x}_t, t) - \boldsymbol{x}_0 \right\|_2^2 \right]\] <p>In many popular implementations, DDPM, improved DDPM, DDIM, etc., people even simplify this further to directly predict the noise $\epsilon$ instead of $\hat{\boldsymbol{x}}_0$, leading to the now-standard simple loss:</p> \[L_{\text{simple}} = \mathbb{E}_{t, \boldsymbol{x}_0, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \right\|_2^2 \right]\] <h2 id="score-based-diffusion">Score-based Diffusion</h2> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"kasjfksj/kasjfksj.github.io","data-repo-id":"R_kgDOMlIRVQ","data-category":"Announcements","data-category-id":"DIC_kwDOMlIRVc4Ch8ft","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Xiaoyang Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-people",title:"people",description:"members of the lab or group",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"post-diffusion-drifting-from-score-point-of-view",title:"Diffusion Drifting - from score point of view",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/drifting/"}},{id:"post-diffusion-models-for-image-compression-and-transmission",title:"Diffusion Models for Image Compression and Transmission",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/compression/"}},{id:"post-frequency-analysis-in-images-and-diffusion-models",title:"Frequency Analysis in Images and Diffusion Models",description:"Exploring how low- and high-frequency components reveal image structure and how diffusion models progressively build details.",section:"Posts",handler:()=>{window.location.href="/blog/2026/freq/"}},{id:"post-diffusion-series-video-diffusion-for-dmlab-maze-navigation",title:"Diffusion Series - Video Diffusion for DMLab Maze Navigation",description:"Comparing reconstruction-guided sampling and direct noised-frame injection for improving temporal consistency in long video generation",section:"Posts",handler:()=>{window.location.href="/blog/2026/video-guidance/"}},{id:"post-improving-zod-mc",title:"Improving ZOD-MC",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/zod/"}},{id:"post-diffusion-series-ddpm-model",title:"Diffusion Series - DDPM model",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion/"}},{id:"post-uiuc-summer-research-from-accidental-discord-find-to-lab-assistant",title:"UIUC Summer Research From Accidental Discord Find to Lab Assistant",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/UIUC-summer/"}},{id:"post-ito-39-s-integral",title:"Ito&#39;s Integral",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/Ito/"}},{id:"post-sde",title:"SDE",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/SDE/"}},{id:"post-grokking-a-possible-way-to-achieve-agi",title:"Grokking - a possible way to achieve AGI",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Analysis-Grokfast/"}},{id:"post-grokking-a-possible-way-to-achieve-agi",title:"Grokking - a possible way to achieve AGI",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Grokking/"}},{id:"post-history-of-position-encoding",title:"History of Position Encoding",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/position-encoding/"}},{id:"post-lora-a-potential-parameter-efficient-fine-tuning-method-for-large-models",title:"LoRA - a potential parameter efficient fine-tuning method for large models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/LoRAa/"}},{id:"post-summer-class-experience",title:"Summer class experience",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/UCB-summer/"}},{id:"projects-video-diffusion",title:"Video diffusion",description:"",section:"Projects",handler:()=>{window.location.href="/projects/video_diffusion/"}},{id:"projects-partial-zod-mc",title:"Partial zod mc",description:"",section:"Projects",handler:()=>{window.location.href="/projects/zod_mc/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%78%69%61%6F%6C%36%36@%75%63%69.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>