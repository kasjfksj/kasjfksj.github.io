<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kasjfksj.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kasjfksj.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-23T18:44:26-07:00</updated><id>https://kasjfksj.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">UIUC Summer Research From Accidental Discord Find to Lab Assistant</title><link href="https://kasjfksj.github.io/blog/2025/UIUC-summer/" rel="alternate" type="text/html" title="UIUC Summer Research From Accidental Discord Find to Lab Assistant"/><published>2025-06-22T06:43:40-07:00</published><updated>2025-06-22T06:43:40-07:00</updated><id>https://kasjfksj.github.io/blog/2025/UIUC-summer</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2025/UIUC-summer/"><![CDATA[<h1 id="introduction">​​Introduction​​</h1> <p>Here, I want to share my experience conducting summer research at UIUC (University of Illinois Urbana-Champaign) this summer. The entire journey was quite unexpected, and I learned so much!</p> <h1 id="an-accidental-start">​An Accidental Start​​</h1> <p>It happened completely by chance. Around March, I was casually browsing our UCI (University of California, Irvine) Discord server when I saw someone sharing a post about UIUC’s summer research program recruiting software engineering students. Honestly, I barely knew what software engineering was back then—I just thought it involved coding? It seemed worlds away from my background. But for some reason, maybe because I was bored that day or just thought, “Why not give it a try? Nothing to lose,” I clicked the link and filled out the application. I had zero expectations, figuring it was a long shot anyway since I had no relevant background.</p> <h1 id="surprise-i-got-shortlisted">​Surprise: I Got Shortlisted!​​</h1> <p>After submitting the application, I nearly forgot about it. Then, over a month later, I received an email! It said I was under consideration! However… there were 1,500 other candidates on the shortlist! Seeing that number made my heart sink—it felt like a lost cause. But! The email stated admission required completing a task: reading two software engineering papers and writing a critical review/summary.</p> <p>Seeing this task actually gave me a glimmer of hope! It meant they were sincerely evaluating applicants, not just randomly screening resumes. Sure, the competition was fierce, but at least I had a fighting chance. I’d already been rejected by two other summer programs, which was really discouraging, so this time I felt I had to seize this opportunity! So, this complete software engineering novice sat down and tackled the two papers. Honestly, they were confusing at first—so much terminology I didn’t recognize. I slowly looked things up one by one and spent ages writing my summaries. Finally… I actually received the acceptance email! Only 7 were accepted out of 1,500+ applicants! I was super excited! It felt like all the effort had paid off, and the disappointment from earlier rejections instantly vanished.</p> <p>​​First Meeting: Nerves and Surprises​​ The program began, and it was time for the first online meeting. When it was my turn to introduce myself, I was incredibly nervous—my heart felt like it was leaping out of my chest. But I managed to stammer through explaining my background. After the meeting, the professor (who is female) added, “Oh by the way, I’m a UCI alum too!” Wow, what a surprise! It instantly made me feel closer to her, like finding a little alumni connection, which greatly eased my nerves.</p> <h1 id="group-work-proactivity-pays-off">​Group Work: Proactivity Pays Off​​</h1> <p>After the meeting, the professor planned to divide us into two groups based on research background and interests, focusing on different directions. But with the professor being busy, three days passed without the groups being formed. Without groups finalized, we couldn’t confirm research topics, so I took the initiative to propose groupings. The professor then followed up quickly, finalized the groups, and assigned our first task: reading four papers.</p> <p>I immediately thought that since we were working together, we needed a place to share materials and discuss. So, I proactively asked in the group chat: “Should I set up a GitHub repository? Maybe also a Docker environment to make it easier for everyone to run code?” Creating the repo, setting up permissions, and writing a simple Docker setup guide—all were done very quickly.</p> <p>Then came reading the papers and writing reports. I pushed myself to finish them ASAP, striving to understand the content. Then, I was the first to submit reports for all four papers to GitHub. I figured since I was doing this, I might as well be proactive. Through reading the four papers and the project kick-off documents the professor shared, I gradually gained clarity about the summer project’s goal: The professor already had a tool to translate C programs to Rust, but needed comprehensive test cases to verify translation accuracy. I shared this understanding with the professor and got a very positive response: “​​You are so right​​,” acknowledging my perspective.</p> <h1 id="unexpected-upgrade-joining-the-lab-early">​Unexpected Upgrade: Joining the Lab Early!​​</h1> <p>Unbelievably, because of these proactive steps (pushing for group formation, setting up GitHub/Docker, being the first to submit reports, and correctly identifying the project’s goal), the professor replied directly, saying I had ​​passed the preliminary evaluation early!​​ I could ​​officially join their lab immediately!​​ This news genuinely shocked and delighted me! I never imagined joining a real lab project so quickly—it felt like winning the lottery!</p> <h1 id="starting-the-lab-work-assisting-the-team">​Starting the Lab Work: Assisting the Team​​</h1> <p>The professor introduced me to the lab members, but they all seemed too busy to respond initially.</p> <p>Based on my understanding of the project’s goal—generating comprehensive test cases to verify the accuracy of C-to-Rust translation—I started diving deep into possible solutions. After researching, I discovered that some test case generation tools and validation scripts could be used. I compiled this research into a feasibility report and shared it. Unexpectedly, this drew ​​significant attention from the professor and other project members​​. Clearly, I had brought a new perspective to the project. Knowing my effort was helping the team made me incredibly excited.</p> <p>Honestly, the initial pressure was intense—immersing myself in practical coding and a real-world project. But being able to genuinely contribute to research felt amazing! Especially seeing my investigation or solutions I helped develop being used in the project—that feeling of “I actually helped” was truly rewarding. Sure, I got stuck and made mistakes sometimes, but what I learned was immense. It felt like progress every day.</p> <h1 id="conclusion-a-precious-unexpected-experience">​Conclusion: A Precious, Unexpected Experience​​</h1> <p>This incredible summer is still ongoing and truly feels wonderful. From randomly spotting a post on Discord and applying on a whim, to stumbling through reading papers, to being pulled straight into the lab early just for being proactive, to having my efforts help the lab… the whole process has been full of surprises and joy.</p> <p>The biggest takeaway: ​​When an opportunity comes, whether you feel ready or not—go for it!​​ Once you’re in a team, be proactive and put in extra effort—it pays off. Even though I knew nothing about software engineering initially, this summer research truly immersed me in the field. I learned practical skills and met incredibly smart people. This summer research experience at UIUC will absolutely be the most valuable and unforgettable highlight of my year.</p>]]></content><author><name></name></author><category term="experience"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[​​Introduction​​ Here, I want to share my experience conducting summer research at UIUC (University of Illinois Urbana-Champaign) this summer. The entire journey was quite unexpected, and I learned so much!]]></summary></entry><entry><title type="html">Understanding Attention and Multi-Head Attention - From Basics to RoPE Optimization</title><link href="https://kasjfksj.github.io/blog/2025/attention/" rel="alternate" type="text/html" title="Understanding Attention and Multi-Head Attention - From Basics to RoPE Optimization"/><published>2025-03-20T09:15:09-07:00</published><updated>2025-03-20T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2025/attention</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2025/attention/"><![CDATA[<h1 id="intro">Intro</h1> <p>In recent years, Transformer models have achieved tremendous success in the field of Natural Language Processing (NLP), with the Attention mechanism being a core component. This article will delve into the principles of the Attention mechanism and its extension, Multi-Head Attention, while introducing an optimization method—Rotary Position Embedding (RoPE), which significantly improves model performance.</p> <h1 id="core-principles-of-the-attention-mechanism">Core Principles of the Attention Mechanism</h1> <p>The Attention mechanism was originally used to address long-range dependency issues in sequence-to-sequence (Seq2Seq) models. Its core idea is that, when generating each output, the model can “dynamically focus” on different parts of the input sequence rather than relying on fixed context.</p> <h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h2> <p>In Transformers, Attention is formalized as Scaled Dot-Product Attention. Given query matrices (Query), key matrices (Key), and value matrices (Value), the computation process is as follows:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Where:<br/> \(Q \in \mathbb{R}^{n \times d_k}\) :Query matrix,<br/> \(K \in \mathbb{R}^{m \times d_k}\) :Key matrix, <br/> \(V \in \mathbb{R}^{m \times d_v}\) :Value matrix, <br/> \(d_k\) :Dimension of the key vector,<br/> \(\sqrt{d_k}\) :Scaling factor to prevent excessively large dot product results.<br/></p> <h2 id="dynamic-weight-allocation">Dynamic Weight Allocation</h2> <p>Attention calculates the similarity between queries and keys to generate a weight matrix, then performs weighted summation over the value matrix. This process can be decomposed into:</p> <p>Similarity Calculation: \(QK^T\) measures the match between queries and keys, Scaling and Normalization: Divide by \(\sqrt{d_k}\) and normalize using softmax. Weighted Summation : Apply the normalized weights to the value matrix \(V\) to generate the final output.</p> <h1 id="multi-head-attention-the-power-of-parallelization">Multi-Head Attention: The Power of Parallelization</h1> <p>To enhance the model’s expressive power, Transformers introduced Multi-Head Attention. Its core idea is to capture information from different subspaces through multiple independent Attention heads.</p> <h2 id="mathematical-form-of-multi-head-attention">Mathematical Form of Multi-Head Attention</h2> <p>Linear Transformation : For each head, map \(Q\), \(K\), \(V\) to different subspaces: \(Q_i = QW_i^Q, \quad K_i = KW_i^K, \quad V_i = VW_i^V\)</p> <p>Where<br/> \(W_i^Q \in \mathbb{R}^{d_k \times d_k}\),<br/> \(W_i^K \in \mathbb{R}^{d_k \times d_k}\),<br/> \(W_i^V \in \mathbb{R}^{d_v \times d_v}\) are learnable weight matrices.<br/></p> <p>Parallel Computation of Attention : Each head independently computes Attention:<br/> \(\text{head}_i = \text{Attention}(Q_i, K_i, V_i)\)</p> <p>Concatenation and Output Transformation : Concatenate the outputs of all heads and apply a linear transformation:<br/> \(\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O\)</p> <p>Where:<br/> \(W^O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}}\): Output weight matrix,<br/> \(h\): Number of heads.<br/></p> <h2 id="why-use-multi-head">Why Use Multi-Head?</h2> <ol> <li>Multi-Perspective Modeling : Each head learns different feature patterns (e.g., syntax, semantics).</li> <li>Enhanced Robustness : Avoids overfitting noise with a single Attention head.</li> <li>Experimental Validation : In machine translation tasks, Multi-Head Attention reduces perplexity by approximately 20% compared to single-head Attention.</li> </ol> <h1 id="rotary-position-embedding-rope-revolutionizing-position-encoding">Rotary Position Embedding (RoPE): Revolutionizing Position Encoding</h1> <p>Traditional Transformers use absolute position encoding (e.g., sine functions) to introduce sequence order information, but modeling positional relationships in long sequences remains limited. RoPE incorporates positional information into Attention calculations via rotation matrices, significantly improving performance.</p> <h2 id="mathematical-definition-of-rope">Mathematical Definition of RoPE</h2> <p>For positions \(m\) and \(n\)，RoPE transforms query and key vectors using rotation matrices \(R_m\) and \(R_n\):</p> \[\boldsymbol{q}' = R_m\boldsymbol{q}, \quad \boldsymbol{k}' = R_n\boldsymbol{k}\] <p>Where the rotation matrix\(R(m)\) is defined as:</p> \[\mathbf{R}_t = \begin{pmatrix} \cos t\theta_1 &amp; -\sin t\theta_1 &amp; \cdots &amp; 0 \\ \sin t\theta_1 &amp; \cos t\theta_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \cos t\theta_{d/2} &amp; -\sin t\theta_{d/2} \\ 0 &amp; 0 &amp; \cdots &amp; \sin t\theta_{d/2} &amp; \cos t\theta_{d/2} \end{pmatrix}\] <h2 id="physical-meaning-of-rope">Physical Meaning of RoPE</h2> <p>Relative Position Encoding: Rotation operations make Attention scores depend only on relative positions \(m - n\)</p> <p>Theoretical Guarantee : RoPE satisfies translational invariance for position encoding \(\mathbf{q}_m' \cdot \mathbf{k}_n' = f(m - n)\).</p> \[\mathbf{q}'_m \cdot \mathbf{k}'_n = \sum_{i=1}^{d/2} \left[ q_m^{(2i-1)}k_n^{(2i-1)} + q_m^{(2i)}k_n^{(2i)} \right]\cos((m-n)\theta_i) \\ + \left[ q_m^{(2i)}k_n^{(2i-1)} - q_m^{(2i-1)}k_n^{(2i)} \right]\sin((m-n)\theta_i)\] <h2 id="experimental-results">Experimental Results</h2> <p>Using RoPE, the language model’s perplexity dropped significantly from 144 to 97 (a reduction of 32.6%), especially effective in long-text generation tasks.</p> <h1 id="workflow-explanation">Workflow explanation</h1> <h2 id="parameter-initialization-deep-dive">Parameter Initialization Deep Dive</h2> <p>Firstly, we initialize our Q, K, V matrices and the matrix \(W_O\):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># d_model is the dimension of word embedding 
</span><span class="n">self</span><span class="p">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="mf">0.01</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="o">*</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)))</span>
<span class="n">self</span><span class="p">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="mf">0.01</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)))</span>
</code></pre></div></div> <p>Why Combined QKV Matrix instead of using separate 3 matrices?</p> <p>• Architectural Efficiency: This allows for single contiguous memory block and better hardware utilization • Implementation Insight: Common pattern in modern transformers (e.g., GPT-2) vs older separate projections (original Transformer paper)</p> <p>Dimension of Input:<br/> • Input: (B, S, D) (Batch × Sequence × ModelDim)<br/> • Projection: (3D, D) matrix transforms D → 3D features<br/> • Output after x @ qkv.T: (B, S, 3D)<br/> Initialization Scale:<br/> • 0.01*randn keeps initial weights small to prevent large softmax gradients early in training<br/> <br/> <br/></p> <ol> <li>QKV Projection &amp; Splitting</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">QKV</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">qkv</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># (B, S, D) -&gt; (B, S, 3D)
</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="n">QKV</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Each with (B, S, D)
</span></code></pre></div></div> <p>Visualization: For D=512:<br/> Input x: [Batch, SeqLen, 512]<br/> QKV: [Batch, SeqLen, 1536] # 3×512<br/> Split → [Batch, SeqLen, 512] each for Q/K/V<br/> Design Choice: Single projection vs multiple:<br/> • Pro: Reduces memory fragmentation<br/> • Con: Limits flexibility in head-specific projections<br/> <br/></p> <ol> <li>Multi-Head Splitting Mechanics</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># H: the number of heads; d_h: dimension for each head
</span><span class="n">dh</span> <span class="o">=</span> <span class="n">D</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span>  <span class="c1"># Head dimension
</span><span class="n">q_heads</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dh</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, S, D) -&gt; (B, S, H, d_h) -&gt; (B, H, S, d_h)
</span>
</code></pre></div></div> <p>Example with Numbers:<br/> • Let D=8, H=2 (heads) → dh=4<br/> • Original Q: (2, 5, 8) (Batch=2, Seq=5)<br/> • After view: (2, 5, 2, 4)<br/> • Transpose: (2, 2, 5, 4) (Head dimension at position 1)<br/> Why Transpose?<br/> • Aligns dimensions for batch matrix multiply in attention:<br/> o (B, H, S, dh) @ (B, H, dh, S) → (B, H, S, S)<br/> Head Specialization:<br/> • Each head processes dh-dimensional subspace (e.g., 512-dim → 64×8 heads)<br/> • Enables capturing diverse linguistic features:<br/> o Head 1: Subject-verb agreement<br/> o Head 2: Pronoun references<br/> o Head 3: Temporal relationships<br/></p> <ol> <li>Attention Mask Construction <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">S_full</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="n">past_length</span><span class="p">)</span>
<span class="n">sq_mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">==</span> <span class="mi">1</span>  <span class="c1"># Convert to boolean
</span></code></pre></div> </div> </li> </ol> <p>Causal Mask Visualization (S=3):<br/> [[1 0 0]<br/> [1 1 0]<br/> [1 1 1]]<br/> • Prevents attending to future positions in autoregressive generation<br/> • Why Not Full Masking? Preserves parallel computation during training<br/> <br/></p> <ol> <li>Scaled Dot-Product Attention Core <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span>
 <span class="n">q_heads</span><span class="p">,</span> <span class="n">k_heads</span><span class="p">,</span> <span class="n">v_heads</span><span class="p">,</span>  <span class="c1"># Shapes: (B, H, S, dh)
</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">sq_mask</span>
<span class="p">)</span>
</code></pre></div> </div> </li> </ol> <p>Under the Hood:<br/></p> <ol> <li>Score Calculation:<br/> Scores=dhQKT<br/> • Scaling prevents gradient saturation in softmax<br/></li> <li>Masked Softmax:<br/> AttentionWeights=softmax(Scores+MaskBias)<br/> • Where MaskBias = -∞ for masked positions<br/></li> <li> <p>Value Weighting:<br/> Output=AttentionWeights⋅V<br/> Optimization: Uses fused CUDA kernel for:<br/> • Memory-efficient attention computation<br/> • Automatic mixed-precision support<br/></p> </li> <li>Output Projection &amp; Head Merging</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>  <span class="c1"># (B, S, H, dh) → (B, S, D)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">wo</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># Final projection
</span></code></pre></div></div> <p>Dimension Restoration:<br/> After attention: (B, H, S, dh)<br/> Transpose → (B, S, H, dh)<br/> Reshape → (B, S, H*dh) = (B, S, D)<br/> Why Final Projection (wo)?<br/></p> <ol> <li>Feature Integration: Combines information from all heads<br/></li> <li>Dimension Matching: Ensures output matches original d_model<br/></li> <li>Learnable Mixing: Allows model to emphasize important heads<br/> <br/> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Complete Dimension Flow Table
| Operation         | Input Shape       | Output Shape      | Key Notes               |
| :---------------- | :---------------: | ----------------: | :---------------------- |
| Input             | (B, S, D)         | -                 | Base embeddings         |
| QKV Projection    | (B, S, D)         | (B, S, 3D)        | Single matrix multiply  |
| Q/K/V Split       | (B, S, 3D)        | 3×(B, S, D)       | Chunk on last dim       |
| Head Splitting    | (B, S, D)         | (B, H, S, dh)     | View + transpose        |
| Attention         | 3×(B, H, S, dh)   | (B, H, S, dh)     | Masked softmax          |
| Head Merge        | (B, H, S, dh)     | (B, S, D)         | Transpose + reshape     |
| Output Projection | (B, S, D)         | (B, S, D)         | Final feature mixing    |
</code></pre></div> </div> <p><br/></p> <h1 id="summary-and-outlook">Summary and Outlook</h1> <p>Attention : Captures sequence dependencies through dynamic weight allocation. Multi-Head : Parallelizes modeling of multi-dimensional features, increasing model capacity. RoPE : Innovates position encoding methods, significantly reducing perplexity. Future directions include:</p> </li> </ol> <p>More efficient position encoding methods (e.g., hybrid absolute/relative position encoding). Sparse acceleration of Attention computation (e.g., Longformer, BigBird).</p> <p>To Do: Add architecture diagrams showing: b) Multi-head attention’s parallel processing c) RoPE’s rotation matrix operations</p> <p>proof of translational invariance for position encoding</p> <p>add code to showcase procedure</p> <p>Add training optimization tips: Include real-world impact metrics</p>]]></content><author><name></name></author><category term="model"/><category term="architecture"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Intro In recent years, Transformer models have achieved tremendous success in the field of Natural Language Processing (NLP), with the Attention mechanism being a core component. This article will delve into the principles of the Attention mechanism and its extension, Multi-Head Attention, while introducing an optimization method—Rotary Position Embedding (RoPE), which significantly improves model performance.]]></summary></entry><entry><title type="html">Grokking - a possible way to achieve AGI</title><link href="https://kasjfksj.github.io/blog/2024/Analysis-Grokfast/" rel="alternate" type="text/html" title="Grokking - a possible way to achieve AGI"/><published>2024-10-10T09:15:09-07:00</published><updated>2024-10-10T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/Analysis-Grokfast</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/Analysis-Grokfast/"><![CDATA[<p>As mentioned in previous blog, grokking is a phenomenon that extremely long training time leads to a sharp incease in val accuracy. Some researchers try to find a way to speed up the grokking phenomenon. GrokFast proposed a new optimizer to boost slow-varying component of the gradient as they hypothesized that it was a contributing factor to Grokking phenomenon.</p> <p>Initially, I believe that GrokFast could be significant and was puzzled when there was only 1 citation currently. However, further testing on GrokFast shows that it was not capable enough to speed up generalization on test dataset. Default setup, using 3 layer and 200 hidden layer size can achieve good result on fast grokking, but other setup will result in much slower learning procedure, resulting much later increase in train accuracy and val accuracy. The test play results are presented below</p> <h1 id="grokfast---testing">GrokFast - testing</h1> <p>I first test the default setup mentioned above. The result is quiet amazing, as we can see the much earlier increase in val acc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grok_fast_none.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> No GrokFast </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_em.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> With GrokFast </div> <p>Later, I changed some parameters in GrokFast to test its general capability over all kinds of network. However, the result was not satsifying.</p> <h1 id="grokfast---effect-of-network-depth">GrokFast - effect of network depth</h1> <p>First, I changed the number of layers to 4 and 5 and tested the performance. While train acc and val acc increased simultaneously, the number of steps required to significantly increase train acc and val acc rised from \(10^3\) to around \(10^4\). This phenomenon was quite puzzling, and I speculated that it was due to the magnifying low-varying gradient component.</p> <p>However, there’s still some interesting results of the experiment.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_4layer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 4layer Grokfast experienced sharper increase than 3layer Grokfast </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_5layer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 5layer Grokfast experienced sharper increase than 4layer Grokfast at $$10^4$$ steps </div> <p>We can see that the network experienced sharper increase at \(10^4\) steps.</p> <h1 id="grokfast---effect-of-network-width">GrokFast - effect of network width</h1> <p>Next, I want to test the effect of network width on GrokFast performance, with the setup of 3 layers of network.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/aassets/img/grok_fast/grokfast_128p.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> at around $$10^4$$ steps, the test acc experienced a decrease and then slowly increase at around 5*10^4 steps </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_512p.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> at around $$10^4$$ steps, the test acc experienced a slight decrease and then rapidly increase. </div> <p>Based on the observation above, I conclude that network width can help network regain its val accuracy after the mysterious drop in val acc.</p> <h1 id="combination-with-lora">Combination with LoRA</h1> <p>Due to the phenomenon, I planed to use Grokfast on LoRA. Specifically, using the gradient update code on matrix A and matrix B can work maybe. However, the first test result was confusing.</p> <p>The first setup is traditional 3 layer + 200 hidden size. GrokFast implemented MA optimizer, and here’s the result</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast+LoRA.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>While val acc and train acc climbed up together at around 10^4, val acc suddenly dropped and continued to decrease until the end of training time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/aassets/img/grok_fast/grokfast_ema.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For ema optimizer, the val acc sharply increase at \(10^4\) steps and then increases at a very slow pace.</p>]]></content><author><name></name></author><category term="encoding"/><category term="method"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[As mentioned in previous blog, grokking is a phenomenon that extremely long training time leads to a sharp incease in val accuracy. Some researchers try to find a way to speed up the grokking phenomenon. GrokFast proposed a new optimizer to boost slow-varying component of the gradient as they hypothesized that it was a contributing factor to Grokking phenomenon.]]></summary></entry><entry><title type="html">Grokking - a possible way to achieve AGI</title><link href="https://kasjfksj.github.io/blog/2024/Grokking/" rel="alternate" type="text/html" title="Grokking - a possible way to achieve AGI"/><published>2024-09-10T09:15:09-07:00</published><updated>2024-09-10T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/Grokking</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/Grokking/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Bigger is better. This phrase is so simple to be true, but it has been testified on AI. With the advent of ChatGPT, people realized that improving model’s performance can be so simple as increasing the size of model and dataset. The scaling laws - bigger and better - for model’s parameter and datasets have been studied thoroughly. However, there are few people studying how increasing training period can affect the model’s performance. Traditionally, if the model is trained for too long, it will enter what’s known as overfitting, a phenomenon that the train accuracy stays high while the validation accuracy drops. We would assume that this model failed to generalize over long period of time due to the poor performance on dataset the model has never seen before.</p> <h2 id="grokking">Grokking</h2> <p>But what happens if we keep on training after overfitting period? Will the test accuracy stays low? This is what researchers in Gork paper answer. They first have a small algorithmically generated datasets: binary operation of division mod 97. Then, they train a neural network on it, recording train accuracy and validation accuracy at each step. They found out that train accuracy reaches near 100% at around \(10^3\) steps. Validation accuracy, however, takes \(10^5\) steps to suddenly achieve near 100%. There’s no steady progress of val acc before \(10^5\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/35561725154306_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There’s a limitation in this paper. They only produce this phenomenon on simple dataset generated by modular function, which has strong data pattern. It’s not sure whether this phenomenon works on much more complex dataset, such as image and language. Even if it does work on complex dataset, it will surely take exceedingly long time to train and finally achieve grooking effect.</p> <p>However, this is quite an interesting phenomenon and needs to be studied. If this grokking effect can happen in every model and every training process, it means that we don’t need additional tricks to maintain validation accuracy of the model. We just need the model to train for a long time, and it will generalize automatically. In that case, we are a step closer to AGI.</p> <h2 id="grokking-fast---low-frequent-gradient">Grokking fast - low frequent gradient</h2> <p>As mentioned before, reaching grokking phenomenon can be time-consuming. For a simple dataset produced by modular, it takes \(1000\times\) for val acc to rise compared to train acc. For a larger complex dataset, the time may take too long to observe any rice of val accuracy.</p> <p>The authors in Grokfast aimed to solve this problem by proposing to amplify slow gradients to accelerate grokking effect. They assumed that grokking effect was due to the slow-varying component of the gradient that results in long time grokking. They treat gradient as a discrete signal and apply low-pass filter to acquire gradients with low frequency, i.e. slow-varying. After that, use a hyper parameter to augment slow gradient and add it to total gradient. Overall, this paper proposes an optimization step to help the model achieve grokking faster.</p> <p>There are of course some advantage and disadvantage of this method, which I will discuss later in the blog.</p> <h2 id="conclusion">Conclusion</h2> <p>I believe grokking has the potential to achieve the same impact as the scaling laws. The latter emphasizes on the largeness on model while the first emphasizes on largeness of training time. Combining those two laws, I believe large language model can go further in generalization.</p>]]></content><author><name></name></author><category term="encoding"/><category term="method"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">History of Position Encoding</title><link href="https://kasjfksj.github.io/blog/2024/position-encoding/" rel="alternate" type="text/html" title="History of Position Encoding"/><published>2024-08-24T09:15:09-07:00</published><updated>2024-08-24T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/position-encoding</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/position-encoding/"><![CDATA[<h1 id="background-positional-information">Background-Positional information</h1> <p>Compared with other sequential model like LSTM and RNN, Transformers are much more successful and serve as the foundation of current language models. These transformer-based LLM achieve higher accuracy than other architecture and can scale up easily due to the parallel computing of Transformer. However, Transformers have its flaws. It requires more computation resources than Sequential model with time complexity of \(O (N^2)\). Also, without external help, Transformers can’t acquire positional information that is important when dealing with sequential data.</p> <p>What’s positional information? Positional information is about the order of characters appear. For instance, “I eat fish” is different from “Fish eat I” because order of appearance is different. “I” appears first in the first setence and “fish” appears first in the second sentence, resulting each sentence having different meaning.</p> <p>Transformers can’t acquire the sequential information of words because the way data is passed to the model. For a traditional model like RNN, each data is passed one at a time. For instance, in the sentence “I eat fish,” “I” will be processed first, then “eat”, then “fish.” This way, RNN acquires sequential information of each word.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/recurrent_network.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, for transformers, all words are passed in all at once. The sequential meaning is lost when the model processes data parallelly instead of sequentially. All words in “I eat fish” will be passed into the model at the same time. The model can’t distinquish which words come first, unless we pass in additional positional information about this word.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/Transformer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Of course, we can just simply assign 1, 2, 3, etc to each word position, but researchers found out better ways to encode position of words.</p> <h1 id="absolute-positional-encoding---sinusoidal-positional-encoding">Absolute Positional Encoding - Sinusoidal Positional encoding</h1> <p>Absolute positioinal encoding is first introduced in the paper Attention Is All You Need. It uses trigonometry function, sine and cosine, to encode positions.</p> <p>For a word \(x\) at position t, the positional encoding will be expressed as:</p> \[e_t = \begin{bmatrix} sin(w_0 \cdot t) \\ cos(w_0 \cdot t) \\ sin(w_1 \cdot t) \\ cos(w_1 \cdot t) \\ \vdots \\ sin(w_{d/2} \cdot t) \\ cos(w_{d/2} \cdot t) \end{bmatrix} \ \ w_k = \frac{1}{10000^{\frac{2k}{d}}} \ \ where \ \ d \ \ is \ \ the \ \ dimension \ \ of \ \ the \ \ positional \ \ embedding\] <p>Using trigonometry function has a very good property. Given word \(p_i\) at position i and word \(e_{i+k}\) at position i+k, we can deduce the position between them by taking dot product.</p> \[\begin{split} e_t \cdot e_{t+k} &amp; = \sum_{i=0}^{\frac{d}{2}-1}sin(w_it)sin(w_i(t+k))+cos(w_it)cos(w_i(t+k)) \\ &amp; = \sum_{i=0}^{\frac{d}{2}-1} cos(w_i(t-(t+k))) \\ &amp; = \sum_{i=0}^{\frac{d}{2}-1} cos(-w_ik) \end{split}\] <p>We can see that the final result is only dependent on k, the relative distance between each word.</p> <p>Let’s recall how attention, the main mechanism of Transformer, works with KQV stuffs.</p> \[Attention(Q,K,V) = softmax(\textbf{QK}^T)\textbf{V}\] \[\textbf{Q} = \textbf{W}_Qx\] \[\textbf{K} = \textbf{W}_Kx\] \[\textbf{V} = \textbf{W}_Vx\] <p>We may only focus on \(\textbf{QK}^T\) because that’s where two words interact each other. We now inject sinusoidal positional encoding to input vector, so instead of \(x\), it’s \(x+e\). We then calculate attention score for words in position i and j, the result will be:</p> <p>\(\textbf{W}_Q(x_i+e_i)(\textbf{W}_K(x_j+e_j))^T =\) \(\textbf{W}_Qx_i{x_j}^T\textbf{W}_K+\textbf{W}_Qe_i{x_j}^T\textbf{W}_K+\textbf{W}_Qx_i{e_j}^T\textbf{W}_K+\textbf{W}_Qe_i{e_j}^T\textbf{W}_K\)</p> <p>We can see that the first term doesn’t contain any positional information of two words. The second and third term only contain positional information of 1 word, which alone can’t deduce the relative positional information. Only the fourth one that has the dot product of two positional embedding contain relative position of two words. This piece of information will be helpful to the model to identify the revelance between each words.</p> <p>Questions:</p> <ol> <li>Why do we add positional embedding to input embedding? What about concatenation? a. It may blur semantic meaning of input embedding by interwining positional embedding with semantic content b. It’ll increase dimension of input vector with no significant increase of model’s accuracy</li> <li>Why many people call sinusoidal positional encoding as absolute? They say it can’t learn relative positional information. a. I searched online looking for answers why this encoding method is absolute and can’t acquire distance between words. Sadly, all the blog I have checked so far didn’t contain detailed mathematical formula to prove it. I guess they call it absolute simply because it only takes the input of current word position and no other word positions.</li> </ol> <h1 id="rotary-positional-encoding">Rotary Positional Encoding</h1> <h1 id="conclusion">Conclusion</h1> <p>Positional encoding is one of the most important feature in Transformer. This blog covers one of the encoding method, the Absolute Positional encoding. Many people say about this absolute positional encoding can’t acquire relative distance two words because it’s “absolute.” However, I do some mathematical deduction and show that this encoding method can learn the relative distance between two words. Therefore, it’s useful in acquiring information beside the current position of the word.</p>]]></content><author><name></name></author><category term="encoding"/><category term="method"/><summary type="html"><![CDATA[Background-Positional information]]></summary></entry><entry><title type="html">LoRA - a potential parameter efficient fine-tuning method for large models</title><link href="https://kasjfksj.github.io/blog/2024/LoRAa/" rel="alternate" type="text/html" title="LoRA - a potential parameter efficient fine-tuning method for large models"/><published>2024-08-13T09:15:09-07:00</published><updated>2024-08-13T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/LoRAa</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/LoRAa/"><![CDATA[<h2 id="background">Background</h2> <p>Imagine you are a researcher testing out language models such as Small T5 or LSTM and you have decent but not great amouont of GPUs. You want to finetune the model to test their performances on a few datasets. You train your model on the datasets and get the results. You are satisfied.</p> <p>But what if there are hundreds of datasets you want to test and not only that, you want to test out current LLM like LLama and Mistral that have billions of parameters. It’s impossible to train on your limited GPU since there are so many parameters that need to be updated through back propogation. You need a way to minimize the number of parameter updates without costing too much computation resources. This is where LoRA comes to play.</p> <h2 id="gradient-descent">Gradient Descent</h2> <p>Every AI model, LLM or CNN, is basically a mathematical function that’s based on some parameters \(\theta\). Parameters are usually weight matrix that multiply with input matrix. For most datasets, we have inputs \(X\) and targets \(Y\). For instance, the input can be a sentence and the output is 0 and 1 where 0 represents negative sentiment and 1 represents positive sentiment. We give the model input \(x \ \ and \ \ x \in X\), and it outputs \(\textit{f}(x, \theta)\). However, we wish that the output will be the target \(y \ \ and \ \ y \in Y\). We use gradient descent to minimize the distance between outcomes and desired outcomes.</p> \[W_t = W_{t-1} + \Delta W \ \ where\ \ W\ \ is \ \ parameter \ \ and \ \ t \ \ is \ \ each \ \ iteration\] <p>For the training session, most computation lies in computing gradient, notably matrix multiplication, which will take incredibly long time when the model size scales up. We can handle this amount of computation on a few datasets, but it’ll be too much when there are many datasets to fine tune on.</p> <h2 id="lora">LoRA</h2> <p><a href="https://arxiv.org/abs/2106.09685">LoRA</a> solves this problems by assuming that the ‘change in weights during model adaptation also has a low “intrinsic rank”’, meaning that the updated parameter can be expressed as the multiplication of 2 low rank matrix. “Rank” is a term in Linear Algebra, which is equivalent to new information. Higher the rank the matrix has, more information the matrix contains.</p> <p>The update formula in LoRA is written as follow:</p> \[W = W_0 + AB \ \ where \ \ A\in R^{m\times r} \ \ and \ \ B\in R^{r\times n}. \ \ Here \ \ r\ll m \ \ and \ \ n\] <p>This way, the computation for gradient will be greatly reduced. For instance, suppose the weight matrix \(W\) is 768 by 1024 matrix. When we calculate the gradient for this matrix, we need to compute 768 \(\times\) 1024 parameters. For \(A\) and \(B\) matrix, we can let A be a 768 by 32 matrix and B be a 32 by 1024 matrix. When the model is doing gradient descent, we froze the model’s parameter, so the gradient for the \(W\) is 0. We only need to calculate gradient for \(A\) and \(B\), which has much less parameters than \(W\). Thus, the computation cost reduces significantly.</p> <h2 id="relora">ReLoRA</h2> <p>Currently, LoRA and its variants, QLoRA, MLoRA, etc, focus on fine-tuning models that are fully trained on datasets. Although during the fine-tuning stage, they require less computations, when we take previously fully trained model into account, it still takes quite a lot of computations. Can we apply LoRA as a training method to a model instead of fully training?</p> <p>When we look at the equation in LoRA closely, we can move \(W_0\) to the left and obtain:</p> \[\Delta W = AB \ \ where \ \ \Delta W = W - W_0\] <p>In a way, we can treat the \(AB\) as the gradient of the model, and yes, we can use LoRA to train a model. This idea is explored by ReLoRA.</p> <p>In their paper, they use LoRA as gradient to update model parameter. First, they train the model just like LoRA without updating model’s parameters. After 2000 steps, the matrix A and B will multiply and add to the model’s weight. Then both matrix will be reinitialize and get trained again.</p> <p>There are several advantage of this method. Firstly, this is much more parameter-efficient than fully-trained model. Secondly, according to the paper, ReLoRA outperforms LoRA though still can’t compare with fully trained model. Thus, the idea of using low-rank update for high rank matrix does work.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/35001724304205_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>LoRA is one of the most popular PEFT method for its simplicity and effectiveness. From my intuition, the way LoRA update parameters is similar to how human learns new things. We often uses previous knowledge and updates the knowledge with some adjustment. For instance, we draw inspiration from real numbers and apply the same arithmetic operations on imaginary number with additional rule that \(i \times i = -1\). I believe LoRA is possibly the key component towards continual learning, making the model more knowledgable about the world and therefore, become the world model.</p>]]></content><author><name></name></author><category term="fine-tune"/><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">Summer class experience</title><link href="https://kasjfksj.github.io/blog/2024/UCB-summer/" rel="alternate" type="text/html" title="Summer class experience"/><published>2024-08-09T09:15:09-07:00</published><updated>2024-08-09T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/UCB-summer</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/UCB-summer/"><![CDATA[<p>This summer, I took UC Berkeley’s CS 61A course which was hard as the rumor said, but my journey included more than just academic challenges. It began with a housing issue. I had arranged to stay in a UCB student house, but just before my arrival, the landlord cut my stay short to July 31 instead of August 15, which meant I had to move out before even finishing my course. Moreover, I was worried it might be a scam, so I quickly canceled and had to scramble for new accommodation.</p> <p>With no place to stay, I had to stay in hotel for 6 days, with sales at $99 a night. It was really expensive. I tried to search for a cheaper option. Fortunately, another CS 61A student, W, offered me a room. To my surprise, he was also a UC Berkeley student, which was a huge benefit since he could give me insights into campus life, dining spots, and local tips.</p> <p>As I began my summer course in UC Berkeley’s CS 61A, I felt confident. The early lectures covered basic Python concepts like booleans, while loops, and functions—topics I knew well. The homework and projects were manageable, and I finished them early. I was optimistic about getting an A+.</p> <p>During this time, I explored the campus with W. It was vast and impressive, almost like a small town with magnificient buildings. Since the campus is on a slope, walking around was tiring. I asked W why the university was built on a hill. Why wouldn’t they build on plains? He shrugged and said when people constructed the campus, this hill was probably the only available place for them to build. We also visited some iconic buildings and sculptures donated by alumni decades ago. This whole place is magnificent and historical, which was really amazing. We also checked out local restaurants, dessert store, and stores like Target and Safeway.</p> <p>Everyday, I would get up, shortly prepare breakfast and get dressed, and take No. 88 or 36 bus with W . Before the lecture, I’d visit a small café in the underground area run by a friendly old lady. I would usually order chocolate chip cookies or muffins as snacks during class. Because I went there so often, that lady reckonized me and would prepare the snacks before I even ordered. It was really kind and considerate of her, remembering me and my preference.</p> <p>After the one and a half hours of lecture, we would head downhill to the Subway, where I would get the BEAST. Although it was really salty, probably from the chess, I really liked the mix of sausage, beef, lettuce. It was relatively cheap compared with others. The crunchiness of lettuce and bread, softness of meat and cheese, the meaty flavor and freshness from vegetables all entangled in my mouse, creating a symphony of taste.</p> <p>After that, we would go to the lab or discussion, depending on the day. In fact, there were no real discussions. Most lab and discussion were just about doing homework and projects, which I could easily get bored when I finished my work. After this period finished, it was in the mid afternoon, where we often had relaxation nearby. I would go to the gelato store near campus, and my favorite flavor is tiramisu or, if the store didn’t provide the flavor, coffee. I would just sit outside, eating the dense and rich-flavored ice cream, and watching cars and crowds pass by. Sometimes, we would go to the gym to exercise. W was more into developing his muscle while I just want to keep myself fit, so the climbing and running machine would be my favorite. The above was perhaps the summary of daily life.</p> <p>During my stay with him, W shared his experience with UC Berkeley. He said to me that he majored in History and Political economy, which startled me very much. I had no idea why he would take computer science when history and politics had nothing to do with STEM. He first told me in a tone like an old professor, saying that every fields were related to each other, even though the combination would seem ridulous, but still existed. Of course, he then said, the reason is to learn new skills, especially when social science required data collection and processing. The programming skill would potentially help him get a job. However, UCB was such a top-notch in STEM and its STEM courses, from W’s articulation, were horrendously hard. He had taken CS 61A for 2 times but had to drop from it because he did so bad that it could greatly affect his GPA. I told him I had a really good foundation on Python. He seemed greatly relieved. I was skeptical about his statement at first. Sure, UCB course was challenging, but how hard could CS 61A, an intro course, be?</p> <p>As the course continued, I began to see what W meant about its difficulty. Topics like iterators and generators were completely new to me, and I needed to watch videos to understand them. The sample exams was the most daunting; even with the answers, some questions were confusing. I realized this course was very tough and that I needed to be very careful to get an A. I started preparing for the midterm two weeks in advance, working on exam questions and reviewing all my notes. I began attending tutoring sessions to augment my understanding. The stress increased, and there was even a period that I questioned if computer science was right for me. I might as well just quit the course, I thought, perhaps only UCI courses fit me.</p> <p>Fortunately, I had excellent TAs, tutors, and lecturers. They answered my questions patiently and their encouragement helped me cope with the stress. One tutor inspired me the most. He was a sophomore who had never programmed before taking CS 61A. He practiced past exams multiple times and got his exam score from 50 to 60 and then finally to 90. His hard work motivated me and I believed that I needed to have the same dedication as him and I kept more practicing. Even W was amazed by my dedication when he asked me why I studied hard for this course even though I had relatively good foundation on programming. I told him I wanted to get A+ on this course and also told him about the adamant spirit of that tutor, and he was also inspired to study past exam together with me.</p> <p>Midterm day finally arrived, and I felt restless as soon as I woke up. The exam was scheduled from 7:00 to 9:00 p.m., so I had plenty of time to review. Despite this, I struggled to focus during my practice exams because I was so anxious about the real test. I told W that I would head to school early and walk around. To calm my nerves, I had some Italian spaghetti for lunch and gelato for dessert. I spent most of the time sitting on the grass land, enjoying the warm breeze and cool grass.</p> <p>As the exam time approached, I went to the assigned classroom. The exam started at 7:03 p.m. I felt prepared for the first question because I had done well on first questions in past exam. But when I flipped the page, my heart nearly stopped. Instead of the expected high-order functions or environment diagrams, the first question was about generators and iterators—topics I had just recently learned. The next page had a question about list operations, another difficult topic for me. I panicked for a few minutes at first, trying to figure out the answers, but then I decided to move on to questions I was more confident about. Most questions were fine, and I could understand all the recursion questions. After finishing those, I went back to previous questions, struggling to remember everything about iterators, generators, and list operations. I spent an hour on those two questions but was still unsure about my answers after the exam ended. I felt devastated by the difficulty and told W, “It’s over. I don’t think I’ll get an A. I might just aim for a B.” Just two days later, W told me our exam scores were out. I hesitated to check, fearing a low score. When I finally opened the email, I was shocked to see that I had scored 61.5 out of 64. At first, I thought it might be a mistake since I did poorly on the first two questions. However, I learned that the class did so poorly that the lecturers had to curve the whole class. I was amazed and utterly happy about my result because it meant that I could have 20 points reduction on the final exam, which meant I have more change to get an A+ in the exam.</p> <p>After the midterm, I felt less stressed about the UCB course and had more time to enjoy life. I met a girl who was from community college in San Jose. Every day, she woke up early to catch a train to UCB. She struggled with her classes and stayed at UCB until 9 p.m. for extra tutoring. She complaint to us about facing harassment during her commute, which worried us. We suggested she try to return home earlier, but she said the TAs only posted their schedules in person, not online. We decided to ask the TAs if they could offer online tutoring for her. They agreed, which helped her avoid returning home late. She was grateful and started spending more time with us.</p> <p>As we got to know her, I learned that she was from mainland China, like W and me. We formed a small study group to help her with programming. She was not good at programming compared with W. However, she was hard-working and patient in learning new knowledge. She would repeatedly ask us questions until she fully understood the concept.</p> <p>One time, we enjoyed lunches together at an Italian restaurant. She told us about her background. Her father owned two hotels, providing her family with a good income. She had even thought about inheriting the family business.</p> <p>“Then why did you come to America instead of China?” W asked.</p> <p>The girl then explained, sadly, about COVID-19 pandemic that had hurt her family’s business, causing financial problems. She realized she couldn’t depend on her family and needed to find her own way. That’s why she came to the U.S., hoping to work in the quantization industry. She took CS 61A to gain programming skills that would help her stand out in job applications. “Every field needs CS skills these days,” W said. “Yep, that’s the trend,” I agreed.</p> <p>Soon, the final exam day arrived. W and I decided to get to campus early and I went to the cafe downstairs with W. W had rarely been here and was immediately attracted to a coin jar for donations. The jar had coins from different countries glued to it—Israel, Japan, China, Sweden. An older lady explained that she had been collecting coins from tourists for years, just for fun. We examined each coin with amazement, noting the unique designs that represented different cultures. I was impressed by W’s observation since I had ordered about 20 brownies and hadn’t even noticed the jar.</p> <p>The final exam started at 7 p.m. It turned out to be relatively easy, focusing mainly on Scheme and SQL. Although these were new topics for me, they felt familiar after hours of practices.</p> <p>After finishing final exam, stepping out of the classroom into the crisp evening air at 10 p.m., I felt a deep sense of achievement from completing the challenging course. After 3 days, I got an email saying I scored 89 out of 92, meaning I could get an A for sure, which greatly relieved me. W initially had A-, but his final score was so close to the minimum requirement of an A. He requested a regrade on the exam, and sure enough, the lecturers gave some points on his final exam, and he finally, for the first time, got an A in CS 61A. He was greatly motivated by this and told me that he wanted to take additional CS course like 61B. “Go ahead,” I encouraged. The girl, however, didn’t perform well on the exam and ultimately got a B-. It was nevertheless her first time to code, and she thanked us for helping her out because without our help, she might as well get a C- or even worse.</p> <p>Overall, it had been a tough but incredible experience, unlike anything I had encountered at UCI. I made new friends, traveled to another city, and lived there for two months. More importantly, I gained confidence in tackling difficult challenges, knowing that my positivity could help me overcome any obstacles.</p> <p>Yet, at the night before I left, I couldn’t fell asleep when I realized that I had to say goodbye to everything I had enjoyed: the campus, the course, the TAs, the Italian restaurants, the gelato store, Safeway, and the buses. I would miss them very much after leaving.</p> <p>After I arrived irvine, I got the letter that notified me that I got an A+. I smiled and knew I had achieved my goals.</p>]]></content><author><name></name></author><category term="classes"/><category term="experience"/><summary type="html"><![CDATA[This summer, I took UC Berkeley’s CS 61A course which was hard as the rumor said, but my journey included more than just academic challenges. It began with a housing issue. I had arranged to stay in a UCB student house, but just before my arrival, the landlord cut my stay short to July 31 instead of August 15, which meant I had to move out before even finishing my course. Moreover, I was worried it might be a scam, so I quickly canceled and had to scramble for new accommodation.]]></summary></entry><entry><title type="html">Image construction experiment</title><link href="https://kasjfksj.github.io/blog/2024/VAE/" rel="alternate" type="text/html" title="Image construction experiment"/><published>2024-05-17T09:15:09-07:00</published><updated>2024-05-17T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/VAE</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/VAE/"><![CDATA[<h1 id="ae">AE</h1> <p>Before talking about VAE, first talk about AE model. AE model aims to decompress and compress images via an encoder and a decoder. Its main purpose is for compressing an image, and this structure is effective While this does allow compression and decompression, it misses the potential usage of such model of generating image. Simple idea of generating image is first draw a sample from latent space following a Gaussian distribution, and then use decoder to generate new images based on it. However, the question lies on the image distribution is most likely to differ from Gaussian distribution.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/VAE/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When the model draws a random sample from Gaussian distribution, it’s most likely to map onto a random noise because the encoder doesn’t learn to map image distribution on to Gaussian distribution. The advent of VAE aims to solve this problem.</p> <h1 id="vae">VAE</h1> <h2 id="quick-explanation">Quick explanation</h2> <p>In order to constrain the distribution of images to Gaussian distribution, VAE uses two vectors to represent final Gaussian distributions in latent varaible, \(\mu\) and \(\sigma\), to simplify the constrain. In addition to architectural changes, it adds additional KL loss onto image reconstruction loss in AE to regulate latent distribution to Gaussian distribution, which is \(L={1}/{2}(-1+{\sigma}^2+{\mu}^2-{log {\sigma}}^2)\). Detailed explanation of its loss term, and the application of ELBO will be discussed in the future post.</p> <h2 id="experiment">Experiment</h2> <p>I implemented a simple version of VAE, using only MLP as the encoder and decoder and training on MNIST dataset.</p> <p>First observation is that the number of dimension in mean and deviation vectors has a significant impact on the quality of image generation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_4_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_4_4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_4_10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> From left to right, the dimension in mean vector and deviation vectors is 2, 4, 10 </div> <p>As the latent space dimention increases, the quality of produced image significantly improves.</p> <p>However, when we use images with more details to train the model, such as CIFAR10, the model can’t reconstruct the original images. The reconstructed images are instead very blurry.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_cifar10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_cifar101.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_cifar102.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> I try to improve performance of VAE on CIFAR10 by only training on 200, 100, 25 images. However, the generated images are very blurry. </div> <p>As I change the size of train dataset, there’s an interesting phenomenon regarding image generation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_cifar103.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_cifar105.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/VAE_cifar104.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The model is trained on a single batch of 2 images. The left image is the generated image. </div> <p>We can see that the generated image is like taking the average of the images in a batch. This is probably because when doing back propagation, the model is doing propagation on all images and takes the average of the gradient. Thus, the final image will resemble all the images in the batch.</p> <h1 id="causes">Causes</h1> <p>As I checked from website, I got to know a phenomenon called posterior collpase. Posterior collapse is when signal from input x to posterior parameters is either too weak or too noisy, and as a result, decoder starts ignoring z samples drawn from the distribution by encoder.</p> <p>A more evident phenomenon is when the decoder generates an image regardless of the input. Thus, I tested some images on VAE to see whether they produce the same image despite different inputs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39211730226621_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39221730226633_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39231730226639_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39241730226646_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39251730226653_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The second and fourth images are generated based on latent variable by the first and third images. The the most right one is generated based on a random noise. We can see that the generated images are nearly the same, which satisfies the symptoms of posterior collapse.</p> <p>Next, I try to figure out the cause of such phenomenon. I first check latent variables z for first and third images, making sure that they are not 0 and different. However, they are not nearly 0 and are different from each other, meaning that the problem probably lies on the decoder instead of encoder.</p> <p>I then changed the decoder, adding more linear layers and hoping it can learn more about latent variable. However, it doesn’t work and the generated images are still very blurry and the same for any latent variable z. So, it’s most likely not the fault of decoder.</p> <p>Later, I examine the loss function which has two parts, reconstruction loss and KL loss. econstruction loss measures whether the generated image matches the original image. KL loss measures whether distribution of latent varaible matches standard Gaussian distribution. During training session, I found out that reconstruction loss is usually 0.2 and remains the same during the process. The KL loss is usually a few hundred initially and then rapidly decreasing to 1 or 2. This stark contrast between reconstruction loss and KL loss is possibly the reason why the model neglects image reconstruction.</p> <p>Threfore, I heavily penalized reconstruction loss, multiplying it by 6000. Below are the result after penalizing reconstruction:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39401730233614_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39411730233637_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39421730233650_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/image%20construction/39431730233659_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally, the model is able to generate discernable image based on the original image, but the generated image is still a bit blurry.</p>]]></content><author><name></name></author><category term="encoding"/><category term="method"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[AE]]></summary></entry><entry><title type="html">Diffusion model</title><link href="https://kasjfksj.github.io/blog/2024/diffusion/" rel="alternate" type="text/html" title="Diffusion model"/><published>2024-04-24T09:15:09-07:00</published><updated>2024-04-24T09:15:09-07:00</updated><id>https://kasjfksj.github.io/blog/2024/diffusion</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/diffusion/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>In recent years, diffusion models have emerged as one of the most powerful techniques in the field of generative AI, yielding remarkable results in image, video, and audio synthesis. These models have gained significant attention for their ability to generate high-quality content, often outperforming traditional models like GANs (Generative Adversarial Networks) and VAEs (Variational Autoencoders) in various creative tasks. But what exactly are diffusion models, and why are they causing such a stir in the AI community?</p> <p>In this blog, we’ll dive deep into the fundamentals of diffusion models, explain how they work, and explore their applications in generative tasks. By the end, you’ll have a clear understanding of this cutting-edge model and how it is revolutionizing the world of generative AI.</p> <h1 id="limitations-of-other-models">Limitations of other models</h1> <p>Before diffusion model was proposed, there were already several generative models, but all of them had some limitations</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/diffusion_model/40841731207612_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Comparison of architectures of different models. </div> <ol> <li> <p>Variational Autoencoders (VAEs) Variational Autoencoders (VAEs) are a class of generative models that rely on the variational inference framework. The model learns an encoder-decoder structure, where encoder maps the input data to a probabilistic latent space (usually Gaussian). Decoder maps the latent space back to the data space to generate new samples. VAEs optimize a lower bound on the log-likelihood of the data using the ELBO (Evidence Lower Bound), which includes a reconstruction term and a regularization term that enforces the learned latent distribution to be close to a prior distribution (usually Gaussian). There are 2 limitations of VAE. Firstly, its generated samples may lack sharpness and realism. Secondly, the regularization in VAEs can sometimes result in blurry outputs due to the continuous latent space.</p> </li> <li> <p>Generative Adversarial Networks (GANs) Generative Adversarial Networks (GANs) consist of two neural networks — a generator and a discriminator — that compete against each other in a minimax game. The generator creates synthetic data, and the discriminator tries to distinguish between real and fake data. The goal is for the generator to create data that is indistinguishable from real data, according to the discriminator. There are 2 limitations of GAN. Firstly, GANs has instability during training session due to optimization of minmax game. Secondly ,they are prone to mode collapse, where the model fails to capture the full diversity of the data distribution.</p> </li> <li> <p>Normalizing Flows (NF) Normalizing Flows (NF) are another class of generative models that provide a way to transform simple distributions (e.g., Gaussian) into complex ones via a series of invertible transformations. The key idea is that the model learns a sequence of invertible functions that map a simple distribution to the target data distribution. Normalizing Flows resemble to diffusion models where both of them try to trace from simple distributions into complex data distribution via transformations. However, there are 2 limitations in NF. Firstly, it’s computationally expensive due to the need for invertible transformations, which can make them less scalable for high-dimensional data. Secondly, it’s not flexible when the transformations need to be invertible.</p> </li> </ol> <h1 id="what-is-diffusion-model">What is diffusion model</h1> <p>Diffusion models are a class of generative models that gradually add noise to data, transform it into pure noise, and then learn to recover the data through the reverse diffusion process. This is akin to simulating the way particles diffuse through a medium, but instead of particles, the model learns to diffuse data (like images, audio, etc.).</p> <h1 id="key-insighsts-behind-diffusion-model">Key insighsts behind diffusion model</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/diffusion_model/40831731207587_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>• Forward Process (Noise Addition): This is where the model gradually adds noise to the data, step by step, until the original data is completely turned into noise. This process is typically modeled using a Markov chain.</p> <p>• Reverse Process (Denoising): In the reverse process, the model learns how to recover the original data from the noisy version, step by step. This is where the generative magic happens—by reversing the noise addition process, the model gradually recovers the original signal, ultimately producing clean data from random noise.</p> <p>Diffusion models belong to the broader family of probabilistic models, where the generation of data is formulated as a sequence of probabilistic transitions from a simple distribution (like pure noise) to a complex data distribution (such as real images).</p> <h1 id="how-do-diffusion-models-work">How Do Diffusion Models Work?</h1> <p>To better understand diffusion models, let’s break down the steps involved in both the forward and reverse processes.</p> <h2 id="forward-process-adding-noise">Forward Process (Adding Noise)</h2> <p>In the forward process, we start with a data point (e.g., an image) and progressively add small Gaussian noise over several time steps. As the process continues, the image becomes increasingly noisy until, at the end of the process, it resembles pure Gaussian noise. For an image \(x_0\), at each time step t, noise is added according to the formula \(x_{t+1}=\sqrt{1-\beta_t}x_t+\beta_t\epsilon\). \(\beta_t\) is called noise schedule, which controls how much noise is added at each step, typically starting with a small amount and increasing as time progresses. \(\epsilon\) is random Gaussian noise. For mathematical convenience, we may write \(\alpha_t=1-\beta_t\), and \(x_{t+1}=\sqrt{\alpha_t}x_t+\sqrt{1-\alpha_t}\epsilon\)</p> <p>We can rewrite forward process in terms of probability. In this case, \(q(x_{t+1}\vert x_t)=N(x_{t+1}; \sqrt{\alpha_t}x_t,(1-\alpha_t)I)\).</p> <h2 id="reverse-process-denoising">Reverse Process (Denoising)</h2> <p>Once the forward process is defined, the goal of the diffusion model is to learn the reverse process. The reverse process involves learning how to remove the noise step by step, recovering the original data distribution from the noisy version.</p> <p>Thus, the question becomes acquiring backward probability – \(q(x_t \vert x_{t+1})\). However, if we directly apply bayesian laws, we get \(q(x_t \vert x_{t+1})=\frac{q(x_{t+1} \vert x_{t})q(x_t)}{q(x_{t+1})}\). We don’t know anything about \(q(x_{t+1})\) or \(q(x_t)\).</p> <p>What we can do is using \(x_0\) as additional information in these probabilities. Instead of solving \(q(x_t \vert x_{t+1})\), we solve \(q(x_t \vert x_{t+1},x_0)\). After bayesian laws we get \(q(x_t \vert x_{t+1},x_0) = \frac{q(x_{t+1} \vert x_{t},x_0)q(x_t \vert x_0)}{q(x_{t+1} \vert x_0)}\). \(q(x_{t+1} \vert x_{t},x_0)\) is the same as \(q(x_{t+1} \vert x_{t})\) due to markov chain property.</p> <p>It turns out that \(q(x_{t+1} \vert x_0)\) and \(q(x_t \vert x_0)\) can be solved using reparameterization trick. The mathematical derivation of the trick is below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/diffusion_model/40971731361785_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Essentially, we can write \(q(x_{t+1} \vert x_0)\) and \(q(x_t \vert x_0)\) as \(N(x_{t+1};\sqrt{\bar{\alpha}_{t+1}}x_0, (1-\bar{\alpha}_{t+1})I)\) and \(N(x_t;\sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)\). After some heavy calculations, we can get \(q(x_t \vert x_{t+1},x_0)=N(x_t;\tilde{\mu}_{t+1}(x_{t+1}),\Sigma_q(t+1)I)\). We can rewrite it as \(q(x_{t-1}\vert x_t,x_0)=N(x_{t-1};\tilde{\mu_t}(x_t),\Sigma_q(t)I)\) where \(\tilde{\mu_t}(x_t)=\frac{1}{\alpha_t}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\bar{z}_t)\). \(\Sigma_q(t)=\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\).</p> <h2 id="training-the-diffusion-model">Training the Diffusion Model</h2> <p>In order to train a model, we must define its loss function. By maximizing the likelihood of data distribution \(log p_{\theta}\) and a series of math deduction, we can get the loss function. The full derivation of loss terms is referenced <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html">here</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/diffusion_model/40981731363428_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>First term is called reconstruction term, which measures how well the reconstructed image match the original image.</p> <p>Second term is called prior matching term, which describes how well the final latent space \(p_T\) matches standard Gaussian distribution. Normally we treat it as 0 under the assumption that the final output will be random noise after adding multiple small amount of noises.</p> <p>The third term is called denoising matching term, which matches denoising distribution \(q(x_{t-1}\vert x_t,x_0)\) with model’s prediction \(p_{\theta}(x_{t-1} \vert x_t)\). We can parameterize the mean of model’s prediction as \(p_{\theta}(x_{t-1} \vert x_t)=N(x_{t-1};\tilde{\mu_{\theta}}(x_t,t),\Sigma_q(t)I)\).</p> <p>Recall that \(q(x_{t-1} \vert x_t,x_0)=N(x_{t-1};\tilde{\mu_t}(x_t),\Sigma_q(t)I)\). Applying KL divergence, we can get the loss function \(\frac{1}{2{\Sigma_q(t)}^2}[{ \parallel \mu_{\theta}-\mu_t \parallel}^2]\).</p> <p>The author of the <a href="https://arxiv.org/pdf/2006.11239">paper</a> finds out that we can directly predict noise instead of the mean, which makes the loss function look like this one: \(C[{ \parallel \epsilon_{\theta} - \epsilon_t \parallel }^2]\) where C is a constant.</p> <h2 id="sampling-from-the-diffusion-model">Sampling from the Diffusion Model</h2> <p>Once trained, the model can generate new data by starting with a random noise sample and iteratively denoising it through the reverse process until a realistic sample is produced. As the model predicts the noise at time t, it will subtract that noise from sampled image to get less noiser image.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/diffusion_model/40991731364598_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The training and sampling process in DDPM paper. </div> <h1 id="applications-of-diffusion-models">Applications of Diffusion Models</h1> <p>Diffusion models have been successfully applied across a wide range of domains. Here are a few notable applications:</p> <ol> <li> <p>Image Generation The most prominent use case for diffusion models has been in image generation. Models like DALL·E 2 and Stable Diffusion leverage diffusion techniques to create high-quality, photorealistic images from text descriptions. These models are trained on large datasets of images and learn to generate realistic images by reversing the diffusion process.</p> </li> <li> <p>Super-Resolution and Image Inpainting Diffusion models have also been applied to tasks like image super-resolution (increasing the resolution of low-quality images) and inpainting (filling in missing parts of an image). By learning the reverse diffusion process on images with added noise, these models can recover fine details and generate high-resolution content from low-quality inputs.</p> </li> <li> <p>Audio and Speech Synthesis Diffusion models are also being used to generate audio and speech. By treating sound waves as data and adding noise over time, diffusion models can generate natural-sounding speech or music by reversing the noise process, step by step.</p> </li> <li> <p>Video Generation Generating video frames is a more complex task, but diffusion models are also making strides in this area. Video generation with diffusion models typically involves generating high-quality sequences of frames that are consistent and coherent over time.</p> </li> </ol> <h1 id="why-are-diffusion-models-so-effective">Why Are Diffusion Models So Effective?</h1> <p>There are several reasons why diffusion models have become so popular in generative AI:</p> <ol> <li>Stability: Unlike GANs, which can suffer from training instability, diffusion models tend to be more stable during training, as they optimize a simpler objective—predicting noise at each step rather than 2 objectives of generating and discriminating images.</li> <li>High-Quality and Diverse Output: Due to its direct operations over continuous distributions and the randomness in the diffusion process, it can generate high-quality and diverse images.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/diffusion_model/41001731368455_.pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Experiment results in the paper Denoising Diffusion Probabilistic Models. DDPM outperforms most of the models. </div> <h1 id="challenges-and-future-directions">Challenges and Future Directions</h1> <p>While diffusion models have demonstrated significant promise, they are not without challenges:</p> <p>• Computational Expense: Diffusion models typically require a large number of time steps to generate data, which makes the sampling process computationally expensive. However, researchers are actively working on techniques to reduce this computational cost while maintaining output quality.</p> <p>• Sampling Speed: Due to the iterative nature of the reverse diffusion process, generating data with diffusion models can be slower than other generative models like GANs. There is ongoing research into speeding up the sampling process.</p> <p>• Data Efficiency: Diffusion models tend to require a large amount of training data to perform well, which can be a limitation in some cases.</p> <p>Despite these challenges, diffusion models are rapidly advancing and are likely to remain at the forefront of generative AI research for the foreseeable future.</p> <h1 id="conclusion">Conclusion</h1> <p>Diffusion models represent a fascinating and powerful approach to generative AI, offering high-quality and stable generation of diverse data types. Their ability to model complex distributions through a gradual process of noise addition and removal has revolutionized fields like image synthesis, super-resolution, and audio generation. While there are still challenges to overcome, diffusion models are undoubtedly a key part of the future of generative AI, and we can expect to see even more exciting applications in the coming years.</p> <p>As the field continues to evolve, diffusion models will likely play a crucial role in shaping the next generation of AI-powered creativity and innovation.</p>]]></content><author><name></name></author><category term="model"/><category term="architecture"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>