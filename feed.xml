<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kasjfksj.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kasjfksj.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-25T11:45:48-08:00</updated><id>https://kasjfksj.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Diffusion Drifting - from score point of view</title><link href="https://kasjfksj.github.io/blog/2026/drifting/" rel="alternate" type="text/html" title="Diffusion Drifting - from score point of view"/><published>2026-02-19T06:50:12-08:00</published><updated>2026-02-19T06:50:12-08:00</updated><id>https://kasjfksj.github.io/blog/2026/drifting</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2026/drifting/"><![CDATA[<p>The <a href="https://arxiv.org/abs/2602.04770">Drifting Model</a> introduces a clever training objective for one-step generative models, grounded in the idea of a drifting field that pushes generated samples toward the data distribution. In this post, I want to derive this objective from a more classical angle: starting from KL divergence, passing through score matching, and showing how kernel density estimation (KDE) naturally leads to the mean-shift field that appears at this paper.</p> <h2 id="kl-divergence">KL Divergence</h2> <p>We want \(q_\theta \approx p_r\) where \(p_r\) is the distribution of true images and \(q_\theta\) is the distribution after pushforward model \(q_{\theta} = {f_{\theta}}_{\#} p_{\text{prior}}\) and \(p_{\text{prior}} = \mathcal{N}(0,I)\). The natural objective is:</p> \[D_{\mathrm{KL}}(q_\theta \| p_r) = \mathbb{E}_{\boldsymbol{\epsilon} \sim p_\text{noise}}\!\left[\log q_\theta(f_\theta(\boldsymbol{\epsilon})) - \log p_r(f_\theta(\boldsymbol{\epsilon}))\right]\] <p>We can take monte carlo estimation of the expectation:</p> \[D_{\mathrm{KL}}(q_\theta \| p_r) \approx \frac{1}{N}\sum_{i=1}^{N}\left[\log q_\theta(\mathbf{x}_i) - \log p_r(\mathbf{x}_i)\right], \quad \mathbf{x}_i = f_\theta(\boldsymbol{\epsilon}_i)\] <p>So for each sample \(\mathbf{x}_i\), the KL loss contribution is:</p> \[\ell(\mathbf{x}_i) = \log q_\theta(\mathbf{x}_i) - \log p_r(\mathbf{x}_i)\] <p>Taking gradient w.r.t. each \(\mathbf{x}_i\):</p> \[\nabla_{\mathbf{x}_i}\left[\log q_\theta(\mathbf{x}_i) - \log p_r(\mathbf{x}_i)\right] = \mathbf{s}_q(\mathbf{x}_i) - \mathbf{s}_r(\mathbf{x}_i)\] <p>To reduce the KL, each \(\mathbf{x}_i\) performs gradient descent:</p> \[\mathbf{x}_i \leftarrow \mathbf{x}_i - \eta\bigl(\mathbf{s}_q(\mathbf{x}_i) - \mathbf{s}_r(\mathbf{x}_i)\bigr) = \mathbf{x}_i + \eta\underbrace{\bigl(\mathbf{s}_r(\mathbf{x}_i) - \mathbf{s}_q(\mathbf{x}_i)\bigr)}_{V_{p,q}(\mathbf{x}_i)}\] <p>This is exactly the drifting field \(V_{p,q}(\mathbf{x}) = \mathbf{s}_r(\mathbf{x}) - \mathbf{s}_q(\mathbf{x})\), which is zero iff \(p_r = q_\theta\).</p> <h2 id="kde-makes-scores-sample-based">KDE Makes Scores Sample-Based</h2> <p>However, we only have samples, not densities. For a generated sample \(\mathbf{x} \sim q_\theta\), we approximate \(q_\theta\) using other generated samples \(\{y^-_i\} \sim q_\theta\) as negatives, and the data distribution \(p_r\) using real samples \(\{y^+_j\} \sim p_r\) as positives:</p> \[q_{\theta}(\mathbf{x}) \approx \frac{1}{N}\sum_i k(\mathbf{x}, y^-_i), \qquad p_r(\mathbf{x}) \approx \frac{1}{M}\sum_j k(\mathbf{x}, y^+_j), \qquad k(\mathbf{x}, \mathbf{y}) = \exp\!\left(-\frac{\|\mathbf{x} - \mathbf{y}\|^2}{2\tau^2}\right)\] <p>The KDE score is:</p> \[\nabla_\mathbf{x} \log q_{\theta}(\mathbf{x}) = \frac{\sum_i \nabla_\mathbf{x}\, k(\mathbf{x}, y^-_i)}{\sum_i k(\mathbf{x}, y^-_i)}\] <p>Since \(\nabla_\mathbf{x}\, k(\mathbf{x}, y^-_i) = -\dfrac{\mathbf{x}-y^-_i}{\tau^2} k(\mathbf{x}, y^-_i)\), letting \(\tilde{k}(\mathbf{x}, y^-_i) = \dfrac{k(\mathbf{x}, y^-_i)}{\sum_i k(\mathbf{x}, y^-_i)}\) be the softmax-normalized weights:</p> \[\nabla_\mathbf{x} \log q_{\theta}(\mathbf{x}) = \frac{-\frac{1}{\tau^2}\sum_i k(\mathbf{x}, y^-_i)(\mathbf{x} - y^-_i)}{\sum_i k(\mathbf{x}, y^-_i)} = \frac{1}{\tau^2}\sum_i \tilde{k}(\mathbf{x}, y^-_i)(y^-_i - \mathbf{x}) =: \frac{1}{\tau^2} V^-_q(\mathbf{x})\] <p>This is the mean-shift vector — a weighted average pulling \(\mathbf{x}\) toward nearby generated samples \(\{y^-_i\} \sim q_\theta\). Identically for \(p_r\) with real samples \(\{y^+_j\} \sim p_r\):</p> \[\nabla_\mathbf{x} \log p_r(\mathbf{x}) = \frac{1}{\tau^2}\sum_j \tilde{k}(\mathbf{x}, y^+_j)(y^+_j - \mathbf{x}) =: \frac{1}{\tau^2} V^+_p(\mathbf{x})\] <h2 id="the-drifting-field">The Drifting Field</h2> <p>Substituting into the score difference:</p> \[V_{p,q}(\mathbf{x}) = \mathbf{s}_r(\mathbf{x}) - \mathbf{s}_q(\mathbf{x}) \approx \frac{1}{\tau^2}\!\left(V^+_p(\mathbf{x}) - V^-_q(\mathbf{x})\right)\] <p>This is Eq. (10) of the paper, with \(V^+_p\) attracting \(\mathbf{x}\) toward real data and \(V^-_q\) repelling from generated samples. Anti-symmetry \(V_{p,q} = -V_{q,p}\) follows directly from the score difference structure, guaranteeing \(V_{p,q} = \mathbf{0} \Leftrightarrow p_r = q_\theta\).</p> <p>The two normalized kernels combine into a joint weight (Eq. 11):</p> \[V_{p,q}(\mathbf{x}) = \mathbb{E}_{y^+\sim p_r,\, y^-\sim q_\theta}\!\left[\tilde{k}(\mathbf{x}, y^+)\,\tilde{k}(\mathbf{x}, y^-)\,(y^+ - y^-)\right]\] <h2 id="training-loss">Training Loss</h2> <p>If the model can accurately push the prior distribution to image distribution, at equilibrium \(V_{p,q}(\mathbf{x}) = \mathbf{0}\), so \(\mathbf{x} + V_{p,q}(\mathbf{x}) = \mathbf{x}\). This motivates chasing a frozen drifted target at each training step:</p> \[\mathcal{L} = \mathbb{E}_{\boldsymbol{\epsilon}}\left\|f_\theta(\boldsymbol{\epsilon}) - \operatorname{sg}\!\left(f_\theta(\boldsymbol{\epsilon}) + V_{p,q}(f_\theta(\boldsymbol{\epsilon}))\right)\right\|^2 = \mathbb{E}\!\left[\|V_{p,q}\|^2\right]\] <p>Minimizing \(\mathcal{L}\) drives \(V_{p,q} \to \mathbf{0}\), which by the score equivalence above drives \(q_\theta \to p_r\).</p>]]></content><author><name></name></author><category term="AI"/><summary type="html"><![CDATA[The Drifting Model introduces a clever training objective for one-step generative models, grounded in the idea of a drifting field that pushes generated samples toward the data distribution. In this post, I want to derive this objective from a more classical angle: starting from KL divergence, passing through score matching, and showing how kernel density estimation (KDE) naturally leads to the mean-shift field that appears at this paper.]]></summary></entry><entry><title type="html">Diffusion Models for Image Compression and Transmission</title><link href="https://kasjfksj.github.io/blog/2026/compression/" rel="alternate" type="text/html" title="Diffusion Models for Image Compression and Transmission"/><published>2026-02-06T11:48:23-08:00</published><updated>2026-02-06T11:48:23-08:00</updated><id>https://kasjfksj.github.io/blog/2026/compression</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2026/compression/"><![CDATA[<h1 id="diffusion-models-for-low-bitrate-image-transmission">Diffusion Models for Low-Bitrate Image Transmission</h1> <p>Traditional image compression fails at low bitrates, producing artifacts and blur. Diffusion models offer a solution by using generative priors to reconstruct plausible images from tiny bitstreams. Here I’m introducing one of the simplest way of doing diffusion for image transmitssion.</p> <h2 id="the-core-mechanism">The Core Mechanism</h2> <p>Supposedly you have a pretrained diffusion model that samples from distribution \(p(x_{t-1} \mid x_t)\). In order to retrieve an image from such distribution, we need to recursively apply diffusion process on noisy images to produce a clean image. But to reconstruct a specific image accurately, you need a sample from \(q(x_{t-1} \mid x_t, x_0)\), which is a slightly different distribution that depends on the original image.</p> <p>The question is how to communicate a sample from \(q(x_{t-1} \mid x_t, x_0)\) where you only have access to \(p(x_{t-1} \mid x_t)\)?</p> <p>The simplest way is to use rejection sampling, where:</p> <ol> <li>Sample \(x_{t-1}\) from \(p(x_{t-1} \mid x_t)\)</li> <li>Accept with probability \(\frac{q(x_{t-1} \mid x_t, x_0)}{M \cdot p(x_{t-1} \mid x_t)}\)</li> <li>If rejected, repeat until acceptance</li> </ol> <p>The encoder performs this rejection sampling to find the accepted sample at index \(k\).</p> <p>Now here comes a clever trick. Instead of directly sending such sample, the encoder sends \(k\) to the receiver. The receiver generates samples from \(p(x_{t-1} \mid x_t)\) using the same shared random seed and stops at the \(k\)-th sample, which is the accepted one. Because senders and receivers are using the same seed, it’s guaranteed that the sequence of samples are identical. So the receiver can always get the same sample as the sender.</p> <p>The number of bits required to transmit the index \(k\) depends fundamentally on the divergence between \(q\) and \(p\). The expected number of rejection sampling trials before acceptance is $M$, where $M$ is the rejection sampling constant satisfying $q(x_{t-1} \mid x_t, x_0) \leq M \cdot p(x_{t-1} \mid x_t)$ for all $x_{t}$.</p> <p>The expected value of \(k\) follows a geometric distribution with success probability $1/M$, so \(\mathbb{E}[k] = M\). To encode an index drawn from a geometric distribution, we need approximately:</p> \[\mathbb{E}[\text{bits}] \approx \log_2(M) + 2\] <p>bits on average.</p> <p>More fundamentally, the communication cost is related to the KL divergence between $q$ and $p$:</p> \[D_{KL}(q \ \mid p) = \mathbb{E}_{x_{t-1} \sim q}\left[\log \frac{q(x_{t-1} \mid x_t, x_0)}{p(x_{t-1} \mid x_t)}\right]\] <p>While \(M\) provides an upper bound on the ratio \(q/p\), the KL divergence captures the average information difference. The relationship can be bounded by:</p> \[\log M \geq D_{KL}(q \ \mid p)\] <p>with equality when $q = p$ (in which case $M = 1$ and no communication is needed).</p> <p>For the entire diffusion process from $t = T$ to $t = 0$, the total communication cost is:</p> \[\text{Total bits} \approx \sum_{t=1}^{T} D_{KL}(q(x_{t-1} \mid x_t, x_0) \ \mid p(x_{t-1} \mid x_t))\] <p>This sum represents the total information gap that must be communicated to reconstruct the specific image \(x_0\) from the diffusion model. When the pretrained model \(p\) closely matches the posterior \(q\), the KL divergence is small and very few bits are needed. Conversely, if \(p\) is a poor approximation of \(q\), many more rejection sampling trials are required, increasing the communication cost substantially.</p> <h1 id="uqdm">UQDM</h1> <p>The paper “Progressive Compression with Universally Quantized Diffusion Models” addresses the computational intractability of the above approach when using Gaussian distributions. The key insight is to replace Gaussian noise channels with uniform noise channels, enabling the use of <strong>Universal Quantization (UQ)</strong> instead of exponentially-complex Relative Entropy Coding.</p> <h3 id="forward-process">Forward Process</h3> <p>The forward process starts with a Gaussian initialization and then uses uniform noise channels:</p> \[q(z_T | x) = \mathcal{N}(\alpha_T x, \sigma_T^2 I)\] \[q(z_{t-1} | z_t, x) = \mathcal{U}\left(b(t)z_t + c(t)x - \frac{\Delta(t)}{2}, b(t)z_t + c(t)x + \frac{\Delta(t)}{2}\right)\] <p>where the parameters are chosen to match the moments of Gaussian diffusion:</p> \[b(t) = \frac{\alpha_t}{\alpha_{t-1}} \frac{\sigma_{t-1}^2}{\sigma_t^2}, \quad c(t) = \frac{\sigma_{t|t-1}^2 \alpha_{t-1}}{\sigma_t^2}, \quad \Delta(t) = \sqrt{12} \sigma_{t|t-1} \frac{\sigma_{t-1}}{\sigma_t}\] <h3 id="backward-process">Backward Process</h3> <p>The reverse process uses a learned density model convolved with uniform noise:</p> \[p(z_{t-1}|z_t) = g_\theta(z_{t-1}; z_t, t) \star \mathcal{U}(-\Delta(t)/2, \Delta(t)/2)\] <p>where $g_\theta$ is typically chosen as a Gaussian or logistic distribution:</p> \[g_\theta(z_{t-1}; z_t, t) = \text{Logistic}(b(t)z_t + c(t)\hat{x}_\theta(z_t; t), \sigma_Q^2(t) I)\] <p>The model is trained by minimizing the negative ELBO:</p> \[L(x) = \underbrace{KL(q(z_T|x) \| p(z_T))}_{L_T} + \underbrace{\mathbb{E}[-\log p(x|z_0)]}_{L_{x|z_0}} + \sum_{t=1}^{T} \underbrace{\mathbb{E}[KL(q(z_{t-1}|z_t, x) \| p(z_{t-1}|z_t))]}_{L_{t-1}}\] <h3 id="progressive-compression-algorithm">Progressive Compression Algorithm</h3> <p><strong>Encoding Process:</strong></p> <ol> <li><strong>Initialize:</strong> Draw $z_T \sim p(z_T) = \mathcal{N}(0, I)$ using a shared random seed</li> <li><strong>For each step</strong> $t = T, T-1, \ldots, 1$: <ul> <li>Compute the mean: $\mu_Q = b(t)z_t + c(t)x$</li> <li>Draw uniform noise: $u_t \sim \mathcal{U}(-1/2, 1/2)$</li> <li>Quantize: $k_t = \left\lfloor \frac{\mu_Q}{\Delta(t)} + u_t \right\rceil$</li> <li> <table> <tbody> <tr> <td>Entropy encode and transmit $k_t$ using the discretized $p(z_{t-1}</td> <td>z_t)$</td> </tr> </tbody> </table> </li> <li>Update: $z_{t-1} = \Delta(t)(k_t - u_t)$</li> </ul> </li> <li> <table> <tbody> <tr> <td><strong>Final step:</strong> Entropy encode $x$ using $p(x</td> <td>z_0)$</td> </tr> </tbody> </table> </li> </ol> <p><strong>Decoding Process:</strong></p> <ol> <li><strong>Initialize:</strong> Draw $z_T \sim p(z_T) = \mathcal{N}(0, I)$ using the same shared seed</li> <li><strong>For each step</strong> $t = T, T-1, \ldots, 1$: <ul> <li> <table> <tbody> <tr> <td>Compute parameters of $p(z_{t-1}</td> <td>z_t)$ using the denoising network $\hat{x}_\theta(z_t; t)$</td> </tr> </tbody> </table> </li> <li>Draw the same uniform noise: $u_t \sim \mathcal{U}(-1/2, 1/2)$ using shared seed</li> <li> <table> <tbody> <tr> <td>Entropy decode $k_t$ using $p(z_{t-1}</td> <td>z_t)$</td> </tr> </tbody> </table> </li> <li>Reconstruct: $z_{t-1} = \Delta(t)(k_t - u_t)$</li> <li><strong>Lossy reconstruction (progressive):</strong> $\hat{x}<em>t = \hat{x}</em>\theta(z_{t-1}; t-1)$</li> </ul> </li> <li> <table> <tbody> <tr> <td><strong>Lossless reconstruction:</strong> Entropy decode $x$ using $p(x</td> <td>z_0)$</td> </tr> </tbody> </table> </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Diffusion compression works by sharing randomness between encoder and decoder. Instead of transmitting full samples, you only send indices telling the decoder which sample to pick from a shared random sequence. Combined with a compact content latent, this achieves remarkable compression at ultra-low bitrates.</p>]]></content><author><name></name></author><category term="AI"/><summary type="html"><![CDATA[Diffusion Models for Low-Bitrate Image Transmission]]></summary></entry><entry><title type="html">Frequency Analysis in Images and Diffusion Models</title><link href="https://kasjfksj.github.io/blog/2026/freq/" rel="alternate" type="text/html" title="Frequency Analysis in Images and Diffusion Models"/><published>2026-01-18T09:30:00-08:00</published><updated>2026-01-18T09:30:00-08:00</updated><id>https://kasjfksj.github.io/blog/2026/freq</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2026/freq/"><![CDATA[<h1 id="frequency-analysis-of-images-and-diffusion-models-from-structure-to-details">Frequency Analysis of Images and Diffusion Models: From Structure to Details</h1> <p>As someone interested in generative AI, I’ve been diving into why diffusion models produce such sharp and coherent images. One of the most insightful perspectives comes from frequency analysis: decomposing images into low-frequency (broad structure) and high-frequency (fine details) components using the Fourier transform.</p> <p>This short post summarizes two key observations I made while experimenting:</p> <ol> <li>Different image categories have distinct low/high-frequency energy profiles.</li> <li>During the diffusion denoising process, the ratio of high-frequency to low-frequency energy steadily increases.</li> </ol> <p>The goal is to provide a concise, intuitive bridge between frequency-domain thinking and how diffusion models work.</p> <h2 id="frequency-components-in-natural-images">Frequency Components in Natural Images</h2> <p>Any image can be decomposed via the 2D Discrete Fourier Transform (DFT) into a spectrum where:</p> <ul> <li><strong>Low frequencies</strong> (near the center of the spectrum) capture global structure, large shapes, and broad color variations.</li> <li><strong>High frequencies</strong> (toward the edges) capture edges, textures, and fine details.</li> </ul> <h3 id="scenery-vs-portraits">Scenery vs. Portraits</h3> <p>Natural <strong>scenery</strong> images (landscapes, forests, oceans) are typically rich in intricate, spatially varying details across the entire frame. This leads to:</p> <ul> <li>Relatively <strong>low low-frequency energy ratio</strong> (few large uniform regions)</li> <li><strong>High high-frequency energy ratio</strong> (abundant textures and details)</li> </ul> <p>Here are some representative examples:</p> <figure> <picture> <img src="/assets/img/freq/1png_analysis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Portraits</figcaption> </figure> <p>In contrast, <strong>side-profile portraits</strong> of people usually contain large smooth areas (skin, hair, background) and more localized details. This results in:</p> <ul> <li>Relatively <strong>high low-frequency energy ratio</strong></li> <li><strong>Lower high-frequency energy ratio</strong> compared to complex scenery</li> </ul> <figure> <picture> <img src="/assets/img/freq/2png_analysis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Portraits</figcaption> </figure> <p>These differences become very clear in their Fourier magnitude spectra (log-scaled for better visualization):</p> <p>The scenery spectrum is more spread out toward higher frequencies, while the portrait spectrum concentrates energy near the center.</p> <h2 id="frequency-evolution-in-diffusion-models">Frequency Evolution in Diffusion Models</h2> <p>Diffusion models generate images by starting from pure Gaussian noise and iteratively denoising over hundreds of steps (the reverse process). A striking pattern emerges when we track frequency content over time:</p> <ul> <li><strong>Low-frequency information</strong> (overall layout, major shapes, broad colors) appears and stabilizes relatively early.</li> <li><strong>High-frequency details</strong> (sharp edges, textures, fine patterns) are progressively refined in later steps.</li> </ul> <p>As a result, the ratio of <strong>high-frequency energy to low-frequency energy generally increases</strong> throughout the denoising process.</p> <p>Here is a visualization of how the energy of different frequency evovles in diffusion model’s denoising:</p> <figure> <picture> <img src="/assets/img/freq/frequency_bands_conditional.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Portraits</figcaption> </figure> <p>This coarse-to-fine behavior explains much of the success of diffusion models: they first establish “what the image is about” (low frequencies) before adding crisp details (high frequencies). This progression is similar to how humans perceive and draw scenes.</p> <h2 id="key-takeaways">Key Takeaways</h2> <ul> <li>Scenery images are richer in high-frequency content due to their detailed textures.</li> <li>Human portraits have stronger low-frequency content because of smoother regions.</li> <li>Diffusion models naturally follow a <strong>low-to-high frequency progression</strong>, which is one reason they produce photorealistic results.</li> </ul> <p>Understanding these frequency dynamics can possibly lead to interesting application: frequency-aware losses, adaptive sampling schedules, or even specialized models for different image domains.</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="computer-vision,"/><category term="generative-models"/><summary type="html"><![CDATA[Exploring how low- and high-frequency components reveal image structure and how diffusion models progressively build details.]]></summary></entry><entry><title type="html">Diffusion Series - Video Diffusion for DMLab Maze Navigation</title><link href="https://kasjfksj.github.io/blog/2026/video-guidance/" rel="alternate" type="text/html" title="Diffusion Series - Video Diffusion for DMLab Maze Navigation"/><published>2026-01-16T09:45:00-08:00</published><updated>2026-01-16T09:45:00-08:00</updated><id>https://kasjfksj.github.io/blog/2026/video-guidance</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2026/video-guidance/"><![CDATA[<h2 id="intro">Intro</h2> <p>This post examines the application of video diffusion models to generate 300-frame first-person maze navigation sequences from the DeepMind Lab (DMLab) environment at 64×64×3 resolution. The primary challenge addressed is maintaining temporal consistency across extended sequences.</p> <h2 id="motivation-and-initial-approach">Motivation and Initial Approach</h2> <p>The initial objective was to implement a functional video diffusion model capable of generating coherent long-horizon navigation videos. A 3D U-Net architecture with temporal attention layers was trained on DMLab episodes.</p> <p>However, as I tested reconstruction-guided sampling mechanisms, the generated videos frequently exhibited temporal inconsistencies such as abrupt layout shifts and discontinuous motion.</p> <p>The reconstruction-guided sampling, as described in the Video Diffusion Models, applies a guidance term during the sampling process, utilizing gradients of the Mean Squared Error computed over overlapping frame regions to enforce reconstruction consistency.</p> <p>Although MSE is a convex objective and the guidance formulation is theoretically sound, the resulting videos still showed progressive degradation in temporal fidelity over longer sequences. Multiple hyperparameter adjustments and diagnostic visualizations of intermediate noise estimates confirmed that information from prior frames was not being adequately propagated through the denoising trajectory.</p> <h2 id="proposed-alternative-direct-noised-frame-injection">Proposed Alternative: Direct Noised-Frame Injection</h2> <p>The observed limitations suggested that early denoising steps — where both the noise prediction and partially denoised estimate contain minimal semantic content — may not provide sufficient context for long-horizon dependency modeling.</p> <p>To address this, an alternative conditioning mechanism was implemented: direct injection of noised versions of overlapping previous frames into the input of the current denoising step. This approach explicitly supplies contextual information from prior timesteps, enabling the model to better leverage short-term dependencies when predicting subsequent frames.</p> <p>This method was compared against standard reconstruction-guided sampling using the same trained checkpoint and sampling procedure.</p> <h2 id="experimental-setup">Experimental Setup</h2> <ul> <li><strong>Dataset</strong>: DeepMind Lab maze navigation episodes</li> <li><strong>Resolution &amp; Length</strong>: 64×64×3 per frame, 300 frames</li> <li><strong>Model</strong>: 3D U-Net with temporal attention, trained from scratch in PyTorch</li> <li><strong>Sampling</strong>: DDPM with 1000 steps</li> <li><strong>Evaluation Metrics</strong>: <ul> <li>FID (lower is better): Distribution similarity</li> <li>Temporal LPIPS (lower is better): Frame-to-frame perceptual smoothness</li> <li>Flow Magnitude: Average optical flow strength</li> <li>Flow Consistency: Standard deviation of flow magnitudes (lower is better)</li> </ul> </li> </ul> <h2 id="quantitative-results">Quantitative Results</h2> <table> <thead> <tr> <th>Metric</th> <th>Noised-Frame Injection</th> <th>Reconstruction Guidance</th> <th>Observation</th> </tr> </thead> <tbody> <tr> <td>FID</td> <td>259.22 ± 8.08</td> <td><strong>250.48 ± 19.62</strong></td> <td>Guidance slightly better, higher variance</td> </tr> <tr> <td>Temporal LPIPS</td> <td><strong>0.1450 ± 0.0115</strong></td> <td>0.1790 ± 0.0229</td> <td>Injection substantially smoother</td> </tr> <tr> <td>Flow Magnitude</td> <td>4.35 ± 0.12</td> <td>4.29 ± 0.49</td> <td>Comparable motion intensity</td> </tr> <tr> <td>Flow Consistency (std, ↓ better)</td> <td>2.49</td> <td><strong>2.31</strong></td> <td>Guidance more uniform motion</td> </tr> </tbody> </table> <h2 id="analysis">Analysis</h2> <p>Qualitative inspection revealed that noised-frame injection produced noticeably more fluid navigation sequences with smoother turns and fewer structural artifacts. Reconstruction-guided samples, while marginally sharper overall (reflected in FID), exhibited more frequent small discontinuities, consistent with the elevated Temporal LPIPS scores.</p> <p>These findings support the hypothesis that direct injection of contextual information during early denoising steps can substantially improve temporal coherence, offering a practical alternative to purely gradient-based guidance for long-sequence generation.</p> <p>Neither approach fully resolves the challenges of 300-frame generation at this resolution (high FID values indicate remaining distribution gaps), suggesting that future improvements may benefit from latent-space diffusion, enhanced architectures, or hybrid conditioning strategies.</p> <p>Further experimentation combining elements of both methods is planned.</p> <h1 id="references">References</h1> <ul> <li>Ho, J., et al. (2022). Video Diffusion Models. arXiv:2204.03458</li> <li>DeepMind Lab: https://github.com/google-deepmind/lab</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="diffusion"/><summary type="html"><![CDATA[Comparing reconstruction-guided sampling and direct noised-frame injection for improving temporal consistency in long video generation]]></summary></entry><entry><title type="html">Improving ZOD-MC</title><link href="https://kasjfksj.github.io/blog/2025/zod/" rel="alternate" type="text/html" title="Improving ZOD-MC"/><published>2025-10-07T03:23:12-07:00</published><updated>2025-10-07T03:23:12-07:00</updated><id>https://kasjfksj.github.io/blog/2025/zod</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2025/zod/"><![CDATA[<h1 id="background">Background</h1> <p>At the beginning of the quarter, advised by my professor, I began to dig deep into this paper: Zeroth-Order Diffusion Monte Carlo (ZOD-MC). The idea is fascinating—a way to sample from tricky distributions without needing them to be log-concave or satisfy fancy inequalities like Log-Sobolev. Also, it uses only zeroth-order queries to the potential V, no gradients. It employs denoising diffusion to handle multimodality and even discontinuities, which is cool for real-world stuff like non-convex potentials with high barriers. I feel like this can be a great starting point for studying and researching sampling and diffusion processes. And I started my journey from here.</p> <h2 id="2025-11-13">2025-11-13</h2> <p>Current stage goal: Implementing partial zod mc on independent distribution.</p> <p>Progress and work: I have read zodmc, A Proximal Algorithm for Sampling,</p> <p>But as I read through, the curse of dimensionality hit me hard. The oracle complexity is $\exp(\tilde{O}(d) \log(1/\varepsilon))$, great for low $d$ but very inefficient in high dimensions. In fact, many of the sampling methods it references are either exponential or pseudo-exponential for non-concave functions, or polynomial but concave or with other nice properties. This got me thinking: how to make it better to improve on zero-order sampling? My professor suggested starting with simple cases, like when dimensions are independent or the potential decomposes into lower-dim parts.</p> <p>That sparked the “partial ZOD-MC” idea. If $V(x)$ breaks into sum $V_j(x_j)$ over $b$ blocks, each of size $d/b$, I could run ZOD-MC on each block separately. Parallelize it, concatenate samples. Complexity drops to $O(b) \cdot \exp(\tilde{O}(d/b) \log(1/\varepsilon))$. Feels like a win for independent distributions – tested mentally on separable mixtures, seems efficient.</p> <p>However, implementing the code is quite hard work since the code in the paper only deals with full dimension (log_prob). And no distributions are provided to construct an independent distribution. I need to write log_prob for single dimension and then write custom independent distribution to test my sampling method. After months of efforts, I finally got my method working, and it’s efficient and works perfectly on independent distributions. Which is, although not so significant since it only works on independent distributions, a solid step.</p> <p>Next, I also tried brainstorming other optimizations, like tweaking the rejection sampling proposal. Maybe a better envelope than Gaussian to boost acceptance. But it doesn’t fit at first. Then, I looked more closely into the paper and found out that the whole thing relies on the OU process and Tweedie’s formula, where the conditional is V plus a quadratic – which is why Gaussian is useful and can’t be replaced. Changing it would mess up the score estimation and guarantees from Theorem 1.</p> <h2 id="2025-12-25">2025-12-25</h2> <p>Is it possible to approximate an arbitrary probability distribution using something like a characteristic function expansion?</p> <p>I’ve been looking into series expansions that build on the characteristic function to approximate densities. One well-known approach is the <strong>Edgeworth expansion</strong>, which starts from the standardized normal distribution and adds correction terms based on higher-order cumulants, using Hermite polynomials.</p> <p>The idea is appealing on paper: it’s a systematic expansion that refines the Central Limit Theorem approximation by incorporating skewness and further moments. In cases where the target distribution is reasonably close to normal, the first few terms can provide useful improvements over the plain normal approximation.</p> <p>However, after running some numerical experiments, I found that the Edgeworth expansion performs very poorly when the target distribution has separate modes.</p> <figure> <picture> <img src="/assets/img/zodmc/edgeworth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Edgeworth approximation (dashed line) compared to a bimodal target density (solid line).</figcaption> </figure> <p>As the figure shows, although the expansion does produce 4 modes, it struggles to capture the four distinct peaks, which indicate that it’s poor at approximate distributions with distinctive modes.</p> <p>This failure surprises me at first, but after some retrospection, this has some explanations. The Edgeworth series is fundamentally a perturbation around the Gaussian: each additional term is a polynomial correction multiplied by powers of \(\(1/\sqrt{n}\)\) (in the CLT context). When the underlying distribution is far from unimodal or has strong deviations from normality, the series struggles to converge properly and can exhibit pathological behavior like oscillations or non-positive densities.</p> <p>My tests confirm that these expansions work best for mild departures from normal distribution but are unreliable for multimodal or heavily skewed distributions.</p> <p>This experience has left me skeptical about using Edgeworth-type expansions for general-purpose density approximation, especially when the shape of the target is unknown or complex. For cases involving distinct modes, other methods will almost certainly be needed.</p> <h2 id="2025-01-13">2025-01-13</h2> <p>As I took Math 121B, the professor talked about the eigenvector and eigenvalue which are essentially a transformation of basis such that the linear map will become diagonizable, which will be easier to compute its power and allows to compute its inverse via charateristic polynomial (Cayley–Hamilton).</p> <p>An idea stucked me. What if for the zeroth order sampling, we first find a manifold (Still don’t know what that is) such that the sampling is easy and then do zeroth order sampling on them and then convert back to euclidean space.</p>]]></content><author><name></name></author><category term="math"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Background At the beginning of the quarter, advised by my professor, I began to dig deep into this paper: Zeroth-Order Diffusion Monte Carlo (ZOD-MC). The idea is fascinating—a way to sample from tricky distributions without needing them to be log-concave or satisfy fancy inequalities like Log-Sobolev. Also, it uses only zeroth-order queries to the potential V, no gradients. It employs denoising diffusion to handle multimodality and even discontinuities, which is cool for real-world stuff like non-convex potentials with high barriers. I feel like this can be a great starting point for studying and researching sampling and diffusion processes. And I started my journey from here.]]></summary></entry><entry><title type="html">Diffusion Series - DDPM model</title><link href="https://kasjfksj.github.io/blog/2025/diffusion/" rel="alternate" type="text/html" title="Diffusion Series - DDPM model"/><published>2025-08-19T08:53:28-07:00</published><updated>2025-08-19T08:53:28-07:00</updated><id>https://kasjfksj.github.io/blog/2025/diffusion</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2025/diffusion/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>The purpose writing this blog is to document what I’ve learnt so far about diffusion. To me diffusion is a powerful paradigm and a very interested research area where math (especially probability) collide with AI most. So I want to document everything I’ve learnt so far so that readers can gain better understanding of diffusion model.</p> <p>Since there are so many stuff about diffusion, I will write several blogs on it.</p> <p>The beginning of diffusion model is for image generation. Thus, before discussing diffusion model. First discuss the task of image generation and the probability behind it</p> <h1 id="probability-view-of-image-generation">Probability view of Image generation</h1> <p>The task of image generation is simple: generating realistic images that align with the style of training data, but what does it even mean to “generate realistic images”?</p> <p>Having a strict standard or formula to dictate the realness of an image is not feasible as there are numerous pixels to be considered and the big number of pictures to consider. Therefore, it’s much more appropriate to consider form a probability point of view where we basically want to sample from an unknown probability distribution of real images, denoted as \(p_{data}(x)\).</p> <p>\(x\) with high \(p_{data}(x)\) look like real photographs, x with tiny (near-zero) \(p_{data}(x)\) look like random color patches or just noise.</p> <p>However, such distribution is unknown. Therefore it’s best to have a model that can model data distribution with its own distribution of \(p_{\theta}(x)\) where we want such distribution is as close to \(p_{data}(x)\) as possible.</p> <p>And the objective should be \(\underset{\theta}{argmin} \,\mathcal{F}(p_{data}(x),\,p_{\theta}(x))\) where \(\mathcal{F}\) is the measurement of the distance of two distributions.</p> <p>Of course, the traditional way of measuring distance of 2 distributions is KL divergence, which is given as follow:</p> \[D_{\text{KL}}\!\bigl(p_{\text{data}} \,\ \mid \, p_\theta\bigr) = \mathbb{E}_{x \sim p_{\text{data}}(x)} \!\Bigl[\log p_{\text{data}}(x) - \log p_\theta(x)\Bigr]\] <p><strong>Note:</strong> \(p_{\text{data}}(x)\) is fixed (it’s the real world), so minimizing this KL is <strong>exactly</strong> equivalent to maximizing:</p> \[\mathbb{E}_{x \sim p_{\text{data}}(x)} \!\bigl[\log p_\theta(x)\bigr]\] <h2 id="vae">VAE</h2> <p>However, learning such \(p_{\theta}(x)\) is not so easy since \(x\) typically lies in a very high-dimensional space — for example, a 256×256 color image has dimension 196,608. Modeling the distribution on such high dimension typically causes greater computation and curse of dimension (volume grows exponentially with dimension, requiring exponential number of data to accurately estimate the true distribution of the data).</p> <p>Thus, we want to introduce a model where it generates samples on low-dimensional latent space \(z\), 128 dimension or 512 dimension. We denote such probabilistic model as \(p_\theta(x \mid z) \;=\; \mathcal{N}\big(x \mid \mu_\theta(z),\; \sigma^2 I\big)\) where \(\mu_θ(z)\) is the image output by the decoder network.</p> <p>Also we want the latent space have certain prior, usually \(\mathcal{N}(0,I)\), so that we can easily sampled \(z\). The observed data is then modeled as: \(p_\theta(x) = \int p_\theta(x \mid z) \, p(z) \, dz\)</p> <p>where \(p_\theta(x \mid z)\) is a flexible decoder (typically a deep neural network)</p> <p>You might wonder at this point, why don’t we directly train a model via monte carlo estimation where \(p_\theta(x) \approx \frac{1}{S} \sum_{s=1}^S p_\theta(x \mid z_s),\quad z_s \sim p(z)\)</p> <p>This would work mathematically. But in reality, it performs very bad, because the prior is still in high-dimension, so most sampled \(z_s \sim \mathcal{N}(0,I)\) have tiny (almost zero) likelihood \(p_\theta(x \mid z_s)\)</p> <p>Occasionally, by pure chance, you draw a \(z_s\) that is close to the region where the decoder puts mass on \(x\); then \(p_\theta(x \mid z_s)\) is huge. This is why training model via Monte Carlo estimate from purely Gaussian distribution has insane variance and fails completely.</p> <p>But during training we are not blind — we have the actual data point x as prior. So instead of randomly sample from the prior, we can directly sample \(z\) based on original image \(x\). This can be modeled as conditional distribution \(q_{\phi}(z\mid x)\) which is essentially mapping image distribution to the latent distribution. Such conditional distribution is also parameterized by an encoder network which produces the mean and the variance: \(q_\phi(z \mid x) = \mathcal{N}\bigl(z \;\big|\; \mu_\phi(x),\,\operatorname{diag}(\sigma_\phi^2(x))\bigr)\)</p> <p>With this in mind, we can now rewrite \(\log p_{\theta}(x)\) as the following</p> \[\begin{aligned} \log p_\theta(x) &amp;= \int q_\phi(z \mid x) \log p_\theta(x) \, dz \\ &amp;= \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{p_\theta(z \mid x)} \, dz \\ &amp;= \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \, dz + \int q_\phi(z \mid x) \log \frac{q_\phi(z \mid x)}{p_\theta(z \mid x)} \, dz \\ &amp;= \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \, dz + D_{\text{KL}}(q_\phi(z \mid x) \ \mid p_\theta(z \mid x)) \\ &amp; \geq \int q_\phi(z \mid x) \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \, dz \quad \text{(since KL} \geq 0\text{)} \end{aligned}\] <p>Now rewite this integral form:</p> \[\int q_\phi(z \mid x) \log \frac{p_{\theta}(x \mid z) p(z)}{q_\phi(z \mid x)} dz = \int q_\phi(z \mid x) \log p_{\theta}(x \mid z) dz + \int q_\phi(z \mid x) \log \frac{p_{\theta}(z)}{q_\phi(z \mid x)} dz\] \[= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] - D_{\text{KL}}(q_\phi(z \mid x) \ \mid p_{\theta}(z))\] <p>summing all up:</p> \[\log p_{\theta}(x) \ge \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] - D_{\text{KL}}(q_\phi(z \mid x) \ \mid p_{\theta}(z))\] <p>Remember that our objective is to maximizing</p> \[\mathbb{E}_{x \sim p_{\text{data}}(x)} \!\bigl[\log p_\theta(x)\bigr]\] <p>So the training objective wil become minimizing:</p> \[-\mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] + D_{\text{KL}}(q_\phi(z \mid x) \ \mid p(z)) \quad x \sim p_{data}(x)\] <p>The first term pushes the decoder to reconstruct \(x\) well from encoder samples.</p> <p>The second term forces the encoder to stay close to the standard Gaussian prior.</p> <p>In practice we take one sample \(z \sim q_\phi(z \mid x)\) (reparameterised) for the expectation and compute the KL analytically. (For more details check online description of VAE architecture and the loss)</p> <h2 id="ddpm">DDPM</h2> <p>So far, Variational Autoencoders seems to work well on learning compressed representations of data. They encode an image into a small latent vector and decode it back, while trying to keep the latent codes close to a simple Gaussian distribution \(\mathcal{N}(\mathbf{0}, \mathbf{I})\).</p> <p>The problem is that real-world data (like photos of faces or animals) has a very complicated distribution. Forcing everything through a small bottleneck and a simple Gaussian prior makes it hard for VAEs to capture all the sharp details. As a result, images generated by VAEs often look blurry.</p> <p>So, can we model complex data distributions more accurately?</p> <p>The answer is yes. Instead of trying to jump from simple Gaussian noise to complex data in one huge step (like VAEs do with decoding), we build a smooth probabilistic trajectory connecting the data distribution to a simple Gaussian noise. Each point on this trajectory is a slightly noisier version of the data. By breaking the entire path into many small steps, the change between consecutive points becomes tiny and easy to model. The neural network only needs to learn these small perturbations—predicting and removing a little noise at each step—which is a much simpler task than reconstructing the full image all at once.</p> <p>In practice, we can define a forward process that iteratively adds predetermined Gaussian noise to an image and then try to learn a backward process to convert Gaussian noise to image.</p> <p>The concept is intuitive, but the math that makes it work efficiently is clever and complicated.</p> <h3 id="forward-process-gradually-adding-noise">Forward Process: Gradually Adding Noise</h3> <p>We start with a clean image \(\mathbf{x}_0\). At each timestep \(t = 1\) to \(T\) (usually \(T \approx 1000\)), we add a small independent Gaussian noise:</p> \[\mathbf{x}_t = \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1} + \sqrt{\beta_t} \, \boldsymbol{\epsilon_t}, \quad \boldsymbol{\epsilon_t} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\] <p>Here \(\beta_t\) is a small variance schedule (\(\beta_t \to 1\) as \(t\) increases).</p> <p>To make future mathematical deduction easier, let \(\alpha_t = 1 - \beta_t\), the formula becomes:</p> \[\mathbf{x}_t = \sqrt{\alpha_t} \, \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t}\] <p>Basically, we define the forward process to be: \(p(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \sim \mathcal{N}(\sqrt{\alpha_t} \, \mathbf{x}_{t-1}, 1-\alpha_t)\)</p> <p>Before retrieving backward process \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\), we need to make a mathematical transformation:</p> \[\begin{aligned} \mathbf{x}_t &amp;= \sqrt{\alpha_t} \, \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t} \\ &amp;= \sqrt{\alpha_t} \, (\sqrt{\alpha_{t-1}} \, \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \, \boldsymbol{\epsilon_{t-1}}) + \sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t} \\ &amp;= \sqrt{\alpha_t} \, \sqrt{\alpha_{t-1}} \, \mathbf{x}_{t-2} + \sqrt{\alpha_t} \,\sqrt{1-\alpha_{t-1}} \, \boldsymbol{\epsilon_{t-1}}+\sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t} \end{aligned}\] <p>We can treat \(\sqrt{\alpha_t} \,\sqrt{1-\alpha_{t-1}} \, \boldsymbol{\epsilon_{t-1}}\) and \(\sqrt{1-\alpha_t} \, \boldsymbol{\epsilon_t}\) as representation of Gaussian distributions:</p> \[\begin{aligned} \mathbf{X}_1 &amp;\sim \sqrt{\alpha_t}\,\sqrt{1-\alpha_{t-1}}\,\boldsymbol{\epsilon}_{t-1} = \mathcal{N}\!\left(0,\, \alpha_t(1-\alpha_t)\right) \\ \mathbf{X}_2 &amp;\sim \sqrt{1-\alpha_t}\,\boldsymbol{\epsilon}_t = \mathcal{N}\!\left(0,\, (1-\alpha_t)\right) \end{aligned}\] <p>Since the noise are independently added, \(\mathbf X_1 + \mathbf X_2 \sim \mathcal{N}(0, \, 1-\alpha_t \, \alpha_{t-1})\), so the formula becomes: \(\mathbf{x}_t = \sqrt{\alpha_t \, \alpha_{t-1}} \, \mathbf{x}_{t-2} + \sqrt{1-\alpha_t \, \alpha_{t-1}} \, \epsilon\)</p> <p>Applying this procedure recursively, we’ll get:</p> \[\mathbf{x}_t = \sqrt{\alpha_t \, \alpha_{t-1}\,...\, \alpha_1} \, \mathbf{x}_0 + \sqrt{1-\alpha_t \, \alpha_{t-1} \, ... \, \alpha_1} \, \epsilon\] <p>Let \(\bar{\alpha_t} = \alpha_t \, \alpha_{t-1}\,...\, \alpha_1\), we can simplify the above fomula to be \(\mathbf{x}_t = \sqrt{\bar{\alpha_t}} \, \mathbf{x}_0 + \sqrt{1-\bar{\alpha_t}}\, \epsilon\)</p> <p>The forward process becomes \(p(\mathbf{x}_t \mid \mathbf{x}_0) \sim \mathcal{N}(\sqrt{\bar{\alpha_t}} \, \mathbf{x}_0, 1-\bar{\alpha_t})\)</p> <h3 id="backward-process">Backward Process</h3> <p>In order to get reverse process \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\), we can try to apply bayesian rule:</p> \[p(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \frac{p(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \, p(\mathbf{x}_{t-1})}{p(\mathbf{x}_t)}\] <p>However, the problem is that \(p(\mathbf{x}_t)\) and \(p(\mathbf{x}_{t-1})\) have no explicit formula, so the alternative way is to include the source image \(\mathbf{x}_0\), which rewrites the formula as the following:</p> \[\begin{aligned} p(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) &amp;= \frac{p(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) \, p(\mathbf{x}_{t-1} \mid \mathbf{x}_0)}{p(\mathbf{x}_t \mid \mathbf{x}_0)} \\ &amp;= \exp\left(-\frac{1}{2}\left(\frac{(\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x}_{t-1})^2}{1- \alpha_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\alpha_{t-1}}\mathbf{x}_0)^2}{1 - \bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t}\mathbf{x}_0)^2}{1 - \bar{\alpha}_t}\right)\right) \\ &amp;= \exp\left(-\frac{1}{2}\left(\left(\frac{\alpha_t}{1- \alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right)\mathbf{x}_{t-1}^2 - \left(\frac{2\sqrt{\alpha_t}}{1- \alpha_t}\mathbf{x}_t + \frac{2\sqrt{\alpha_{t-1}}}{1 - \bar{\alpha}_{t-1}}\mathbf{x}_0\right)\mathbf{x}_{t-1} + C(\mathbf{x}_t, \mathbf{x}_0)\right)\right) \end{aligned}\] <p>This seems similar to Gaussian distribution formula. Therefore, we ignore the last term \(C(\mathbf{x}_t, \mathbf{x}_0)\) since \(\mathbf{x}_t, \mathbf{x}_0\) are not relevent to \(\mathbf{x}_{t-1}\).</p> <p>The variance and mean of such probability function is this</p> \[\tilde{\sigma}_t^2 = 1/\left(\frac{\alpha_t}{1- \alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right) = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \alpha_t \\ \tilde{\mu}_t = \left(\frac{\sqrt{\alpha_t}}{1-\alpha_t}\mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_t}}{1 - \bar{\alpha}_t}\mathbf{x}_0\right)/\left(\frac{\alpha_t}{1-\alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)}{1 - \bar{\alpha}_t}\mathbf{x}_0\] <p>Remember that \(\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\epsilon_t\right)\), so we can rewrite the mean to be \(\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\epsilon_t\right)\)</p> <p>Therefore, \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)=\mathcal{N}(\, \tilde{\mu}_t \, , \tilde{\sigma}_t^2)\)</p> <p>However, the distribution \(p(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)\) is not what we want since this implies that we already gain access to generated image \(\mathbf{x}_0\). The ideal distribution that we wish the model to learn is \(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\)</p> <p>This is the backward process and we want the model to learn such distribution.</p> <h3 id="elbo">ELBO</h3> <p>If you remember, for an image generation task, we want to maximize the likelihood:</p> \[\mathbb{E}_{x \sim p_{\text{data}}(x)} \!\bigl[\log q_\theta(x_0)\bigr]\] <p>However, directly maximizing this is intractable. Instead, we derive a tractable lower bound using Jensen’s inequality and properties of the diffusion process.</p> \[\begin{align*} \log q_\theta(\boldsymbol{x_0}) &amp;= \log \int q_\theta(\boldsymbol{x}_{0:T}) d\boldsymbol{x}_{1:T} \\ &amp;= \log \int \frac{q_\theta(\boldsymbol{x}_{0:T}) p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)}{p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)} d\boldsymbol{x}_{1:T} \\ &amp;= \log \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \frac{q_\theta(\boldsymbol{x}_{0:T})}{p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)} \right] \\ &amp;\geq \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_{0:T})}{p(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) \prod_{t=1}^T q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{\prod_{t=1}^T p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1})} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \prod_{t=2}^T q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0) \prod_{t=2}^T p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1})} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \prod_{t=2}^T q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0) \prod_{t=2}^T p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{\frac{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0) p(\boldsymbol{x}_t|\boldsymbol{x}_0)}{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_0)}} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{\frac{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0) \cancel{p(\boldsymbol{x}_t|\boldsymbol{x}_0)}}{\cancel{p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_0)}}} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{\cancel{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)}} + \log \frac{\cancel{p(\boldsymbol{x}_1 \mid \boldsymbol{x}_0)}}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} + \log \prod_{t=2}^T \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T) q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1)}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} + \sum_{t=2}^T \log \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \right] + \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T)}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} \right] + \sum_{t=2}^T \mathbb{E}_{p(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \mathbb{E}_{p(\boldsymbol{x}_1|\boldsymbol{x}_0)} \left[ \log q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \right] + \mathbb{E}_{p(\boldsymbol{x}_T|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_T)}{p(\boldsymbol{x}_T \mid \boldsymbol{x}_0)} \right] + \sum_{t=2}^T \mathbb{E}_{p(\boldsymbol{x}_t, \boldsymbol{x}_{t-1}|\boldsymbol{x}_0)} \left[ \log \frac{q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)}{p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)} \right] \\ &amp;= \underbrace{\mathbb{E}_{p(\boldsymbol{x}_1|\boldsymbol{x}_0)} \left[ \log q_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_1) \right]}_{\text{reconstruction term}} - \underbrace{D_{\text{KL}}(p(\boldsymbol{x}_T \mid \boldsymbol{x}_0) \| q_\theta(\boldsymbol{x}_T))}_{\text{prior matching term}} - \sum_{t=2}^T \underbrace{\mathbb{E}_{p(\boldsymbol{x}_t|\boldsymbol{x}_0)} \left[ D_{\text{KL}}(p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0) \| q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)) \right]}_{\text{denoising matching term}} \end{align*}\] <p>(This derivation is adapted from <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html">Calvin Luo’s excellent diffusion tutorial</a> with minor notation adjustments for consistency with later sections.)</p> <p>In practice, we rarely optimize the full ELBO directly. Instead, most modern diffusion models focus on the following terms:</p> <ul> <li>Reconstruction term → usually approximated via simple noise prediction</li> <li>Denoising matching terms → the main training objective</li> </ul> <p>The prior matching term is typically very small when T is large, so it’s often ignored during training. After some algebra ,the denoising objective for each timestep boils down to this very useful form</p> \[\underset{\theta}{\arg\min} \, D_{\text{KL}}(p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0) \| q_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)) = \underset{\theta}{\arg\min} \frac{1}{2\sigma_q^2(t)} \frac{\bar{\alpha}_{t-1}(1-\alpha_t)^2}{(1-\bar{\alpha}_t)^2} \left[ \left\| \hat{\boldsymbol{x}}_\theta(\boldsymbol{x}_t, t) - \boldsymbol{x}_0 \right\|_2^2 \right]\] <p>In many popular implementations, DDPM, improved DDPM, DDIM, etc., people even simplify this further to directly predict the noise $\epsilon$ instead of $\hat{\boldsymbol{x}}_0$, leading to the now-standard simple loss:</p> \[L_{\text{simple}} = \mathbb{E}_{t, \boldsymbol{x}_0, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \right\|_2^2 \right]\] <h2 id="score-based-diffusion">Score-based Diffusion</h2>]]></content><author><name></name></author><category term="AI"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">UIUC Summer Research From Accidental Discord Find to Lab Assistant</title><link href="https://kasjfksj.github.io/blog/2025/UIUC-summer/" rel="alternate" type="text/html" title="UIUC Summer Research From Accidental Discord Find to Lab Assistant"/><published>2025-06-22T06:43:40-07:00</published><updated>2025-06-22T06:43:40-07:00</updated><id>https://kasjfksj.github.io/blog/2025/UIUC-summer</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2025/UIUC-summer/"><![CDATA[<h1 id="introduction">​​Introduction​​</h1> <p>Here, I want to share my experience conducting summer research at UIUC (University of Illinois Urbana-Champaign) this summer. The entire journey was quite unexpected, and I learned so much!</p> <h1 id="an-accidental-start">​An Accidental Start​​</h1> <p>It happened completely by chance. Around March, I was casually browsing our UCI (University of California, Irvine) Discord server when I saw someone sharing a post about UIUC’s summer research program recruiting software engineering students. Honestly, I barely knew what software engineering was back then—I just thought it involved coding? It seemed worlds away from my background. But for some reason, maybe because I was bored that day or just thought, “Why not give it a try? Nothing to lose,” I clicked the link and filled out the application. I had zero expectations, figuring it was a long shot anyway since I had no relevant background.</p> <h1 id="surprise-i-got-shortlisted">​Surprise: I Got Shortlisted!​​</h1> <p>After submitting the application, I nearly forgot about it. Then, over a month later, I received an email! It said I was under consideration! However… there were 1,500 other candidates on the shortlist! Seeing that number made my heart sink—it felt like a lost cause. But! The email stated admission required completing a task: reading two software engineering papers and writing a critical review/summary.</p> <p>Seeing this task actually gave me a glimmer of hope! It meant they were sincerely evaluating applicants, not just randomly screening resumes. Sure, the competition was fierce, but at least I had a fighting chance. I’d already been rejected by two other summer programs, which was really discouraging, so this time I felt I had to seize this opportunity! So, this complete software engineering novice sat down and tackled the two papers. Honestly, they were confusing at first—so much terminology I didn’t recognize. I slowly looked things up one by one and spent ages writing my summaries. Finally… I actually received the acceptance email! Only 7 were accepted out of 1,500+ applicants! I was super excited! It felt like all the effort had paid off, and the disappointment from earlier rejections instantly vanished.</p> <p>​​First Meeting: Nerves and Surprises​​ The program began, and it was time for the first online meeting. When it was my turn to introduce myself, I was incredibly nervous—my heart felt like it was leaping out of my chest. But I managed to stammer through explaining my background. After the meeting, the professor (who is female) added, “Oh by the way, I’m a UCI alum too!” Wow, what a surprise! It instantly made me feel closer to her, like finding a little alumni connection, which greatly eased my nerves.</p> <h1 id="group-work-proactivity-pays-off">​Group Work: Proactivity Pays Off​​</h1> <p>After the meeting, the professor planned to divide us into two groups based on research background and interests, focusing on different directions. But with the professor being busy, three days passed without the groups being formed. Without groups finalized, we couldn’t confirm research topics, so I took the initiative to propose groupings. The professor then followed up quickly, finalized the groups, and assigned our first task: reading four papers.</p> <p>I immediately thought that since we were working together, we needed a place to share materials and discuss. So, I proactively asked in the group chat: “Should I set up a GitHub repository? Maybe also a Docker environment to make it easier for everyone to run code?” Creating the repo, setting up permissions, and writing a simple Docker setup guide—all were done very quickly.</p> <p>Then came reading the papers and writing reports. I pushed myself to finish them ASAP, striving to understand the content. Then, I was the first to submit reports for all four papers to GitHub. I figured since I was doing this, I might as well be proactive. Through reading the four papers and the project kick-off documents the professor shared, I gradually gained clarity about the summer project’s goal: The professor already had a tool to translate C programs to Rust, but needed comprehensive test cases to verify translation accuracy. I shared this understanding with the professor and got a very positive response: “​​You are so right​​,” acknowledging my perspective.</p> <h1 id="unexpected-upgrade-joining-the-lab-early">​Unexpected Upgrade: Joining the Lab Early!​​</h1> <p>Unbelievably, because of these proactive steps (pushing for group formation, setting up GitHub/Docker, being the first to submit reports, and correctly identifying the project’s goal), the professor replied directly, saying I had ​​passed the preliminary evaluation early!​​ I could ​​officially join their lab immediately!​​ This news genuinely shocked and delighted me! I never imagined joining a real lab project so quickly—it felt like winning the lottery!</p> <h1 id="starting-the-lab-work-assisting-the-team">​Starting the Lab Work: Assisting the Team​​</h1> <p>The professor introduced me to the lab members, but they all seemed too busy to respond initially.</p> <p>Based on my understanding of the project’s goal—generating comprehensive test cases to verify the accuracy of C-to-Rust translation—I started diving deep into possible solutions. After researching, I discovered that some test case generation tools and validation scripts could be used. I compiled this research into a feasibility report and shared it. Unexpectedly, this drew ​​significant attention from the professor and other project members​​. Clearly, I had brought a new perspective to the project. Knowing my effort was helping the team made me incredibly excited.</p> <p>Honestly, the initial pressure was intense—immersing myself in practical coding and a real-world project. But being able to genuinely contribute to research felt amazing! Especially seeing my investigation or solutions I helped develop being used in the project—that feeling of “I actually helped” was truly rewarding. Sure, I got stuck and made mistakes sometimes, but what I learned was immense. It felt like progress every day.</p> <h1 id="conclusion-a-precious-unexpected-experience">​Conclusion: A Precious, Unexpected Experience​​</h1> <p>This incredible summer is still ongoing and truly feels wonderful. From randomly spotting a post on Discord and applying on a whim, to stumbling through reading papers, to being pulled straight into the lab early just for being proactive, to having my efforts help the lab… the whole process has been full of surprises and joy.</p> <p>The biggest takeaway: ​​When an opportunity comes, whether you feel ready or not—go for it!​​ Once you’re in a team, be proactive and put in extra effort—it pays off. Even though I knew nothing about software engineering initially, this summer research truly immersed me in the field. I learned practical skills and met incredibly smart people. This summer research experience at UIUC will absolutely be the most valuable and unforgettable highlight of my year.</p>]]></content><author><name></name></author><category term="CS"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[​​Introduction​​ Here, I want to share my experience conducting summer research at UIUC (University of Illinois Urbana-Champaign) this summer. The entire journey was quite unexpected, and I learned so much!]]></summary></entry><entry><title type="html">SDE</title><link href="https://kasjfksj.github.io/blog/2025/SDE/" rel="alternate" type="text/html" title="SDE"/><published>2025-02-08T10:34:47-08:00</published><updated>2025-02-08T10:34:47-08:00</updated><id>https://kasjfksj.github.io/blog/2025/SDE</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2025/SDE/"><![CDATA[<h1 id="intro">Intro</h1> <p>As a second-year undergraduate in mathematics, I’ve recently started self-studying stochastic differential equations since it’s interesting and applicable to machine learning. While most resources dive quickly into measure-theoretic probability, I found it challenging to find concise, technically accurate summaries that bridge undergraduate calculus/probability and the core machinery of Itô calculus. This post is my attempt to fill that gap. It’s written primarily for fellow undergraduates who know basic probability but haven’t yet taken a full course in stochastic calculus.</p> <h2 id="brownian-motion">Brownian Motion</h2> <p>Before introducing stochastic differential equations, it is helpful to recall the deterministic case.</p> <p>An ordinary differential equation (ODE) has the form</p> \[\frac{df}{dt} = b(f,t), \quad f(t_0) = f_0,\] <p>or in differential notation,</p> \[df = b(f,t)\, dt.\] <p>Here \(b(\cdot,t)\) is the deterministic drift coefficient, and for suitable conditions on \(b\), there exists a unique deterministic solution \(f(t)\).</p> <p>Ordinary differential equations often provide an effective framework for modeling many real-world systems, such as population growth. However, these models are fully deterministic. Given the same initial conditions, they always produce the same trajectory.</p> <p>In reality, most systems are subject to random fluctuations due to environmental noise, measurement errors, or other unpredictable influences. For instance, stock prices are affected by unpredictable market events</p> <p>To capture such randomness in a continuous-time setting, we can extend the deterministic model by adding a stochastic noise term:</p> \[dX_t = \mu(X_t, t)\, dt + \sigma(X_t, t)\, dW_t, \quad X_0 = x_0.\] <p>The noise term is \(\sigma(X_t, t)\, dW_t\), which is driven by increments of a stochastic process \(W_t\)​. In principle, many different processes could be used (for example, increments of a compensated Poisson process to model jump discontinuities). But in application, we often choose standard Brownian motion because it possesses several onvenient properties that make the resulting Itô calculus more tractable and powerful.</p> <p>These key properties include:</p> <ul> <li><strong>Gaussian increments</strong>: \(W_t - W_s \sim \mathcal{N}(0, t-s)\) for \(t &gt; s\),</li> <li><strong>Independent increments</strong>: \(W_{t_{k+1}} - W_{t_k}\) are mutually independent for disjoint intervals,</li> <li><strong>Almost surely continuous paths</strong>: \(t \mapsto W_t(\omega)\) is continuous for almost every \(\omega\).</li> </ul> <p>A crucial consequence of the Gaussian increments property is the scaling of the noise in small time intervals. Over an infinitesimal interval \(dt\), we have</p> \[dW_t = W_{t+dt} - W_t \sim \mathcal{N}(0, dt)\] <p>Thus, \(dW_t\) has mean zero and variance \(dt\), so its typical magnitude (standard deviation) is \(\sqrt{dt}\).</p> <p>This gives rise to a useful informal rule in Itô calculus (which you’ll see soon) is that \((dW_t)^2 \approx dt\), while higher powers like \((dt)^2\) or \(dt \cdot dW_t\) are negligible.</p> <p>These informal rules — \((dW_t)^2 \approx dt\), \(dt \cdot dW_t \approx 0\), \((dt)^2 \approx 0\) — will be made rigorous later, but they hint at why the stochastic integral is very different from traditional calculus.</p> <h2 id="itôs-formula">Itô’s Formula</h2> <p>Let’s suppose there is a function \(f(t,X_t)\) includes random variabes \(X_t\).</p> <p>In order to calculate \(df(t,X_t)\), we can apply second-order Taylor expansion since it includes 2 variables:</p> \[df = \frac{\partial f}{\partial t} \, dt + \frac{\partial f}{\partial x} \, dX + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} (dX)^2 + \frac{\partial^2 f}{\partial t \partial x} \, dt \, dX + \frac{1}{2} \frac{\partial^2 f}{\partial t^2} (dt)^2 + o((dt)^2 + (dX)^2)\] <p>Remember for the multiplication table, \(dt \cdot dt = dt \cdot dW_t = 0\), \(dW_t \cdot dW_t = dt\).</p> <p>Then for an Itô process, we can immediately deduce such formula</p> \[df(t,X_t) = \frac{\partial f}{\partial t}(t,X_t) \, dt + \frac{\partial f}{\partial x}(t,X_t) \, dX_t + \frac{1}{2} \frac{\partial^2 f}{\partial x^2}(t,X_t) \, (dX_t)^2,\] <p>Substituting \(dX_t = \mu \, dt + \sigma \, dW_t\) yields</p> \[df(t,X_t) = \left( f_t + \mu f_x + \frac{1}{2} \sigma^2 f_{xx} \right) dt + \sigma f_x \, dW_t.\] <h2 id="example-geometric-brownian-motion">Example: Geometric Brownian Motion</h2> <p>Consider the SDE for asset prices</p> \[dS_t = \mu S_t \, dt + \sigma S_t \, dW_t, \quad S_0 &gt; 0.\] <p>Apply Itô’s formula to \(f(s) = \log s\):</p> \[f_s = \frac{1}{s}, \quad f_{ss} = -\frac{1}{s^2}.\] <p>Then</p> \[d(\log S_t) = \left( \mu - \frac{1}{2} \sigma^2 \right) dt + \sigma \, dW_t.\] <p>Integrating,</p> \[\log S_t = \log S_0 + \left( \mu - \frac{1}{2} \sigma^2 \right) t + \sigma W_t.\] <p>Thus</p> \[S_t = S_0 \exp\left( \left( \mu - \frac{1}{2} \sigma^2 \right) t + \sigma W_t \right)\] <p>$S_t$ is log-normally distributed with</p> \[\mathbb{E}[S_t] = S_0 e^{\mu t}\] <h2 id="reverse-sde">Reverse SDE</h2> <p>Given SDE \(dX_t = \mu(X_t, t)\, dt + \sigma(X_t, t)\, dW_t, \quad X_0 = x_0.\)</p> <p>This is called the forward process because we can alternatively rewrite this as:</p> \[p(\mathbf{x}_{t+\Delta t} \mid \mathbf{x}_t) = \mathcal {N}(\mu(x_t,t)\, dt, \sigma(x_t, t)^2\, dt) \propto \exp\left(-\frac{(\mathbf{x}_{t+\Delta t} - \mathbf{x}_t - \mu(x_t,t) dt)^2}{2\sigma(x_t, t)^2 dt}\right)\] <p>Now we want to find the reverse process: \(p(\mathbf{x}_t \mid \mathbf{x}_{t+\Delta t}) = \frac{p(\mathbf{x}_{t+\Delta t} \mid \mathbf{x}_t) p(\mathbf{x}_t)}{p(\mathbf{x}_{t+\Delta t})}\)</p> <p>First take the logarithm:</p> \[\log p(\mathbf{x}_t \mid \mathbf{x}_{t+\Delta t}) = \log p(\mathbf{x}_{t+\Delta t} \mid \mathbf{x}_t) + \log p(\mathbf{x}_t) - \log p(\mathbf{x}_{t+\Delta t})\] \[\log p(\mathbf{x}_{t+\Delta t} \mid \mathbf{x}_t) = \log p(\mathbf{x}_t \mid \mathbf{x}_{t+\Delta t}) + \log p(\mathbf{x}_{t+\Delta t}) - \log p(\mathbf{x}_t)\] \[\log p(\mathbf{x}_{t+\Delta t}) = \log p(\mathbf{x}_t) + \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) \cdot (\mathbf{x}_{t+\Delta t} - \mathbf{x}_t) + O(dt)\] \[\log p(\mathbf{x}_{t+\Delta t}) - \log p(\mathbf{x}_t) = \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) \cdot (\mu(x_t,t) dt + \sigma(x_t,t) \sqrt{dt} \, \mathbf{z}) + O(dt)\] <p>substitute back: \(\begin{align} \log p(\mathbf{x}_t \mid \mathbf{x}_{t+\Delta t}) = -\frac{(\mathbf{x}_{t+\Delta t} - \mathbf{x}_t - \mu(x_t,t) dt)^2}{2\sigma(x_t, t)^2 dt} - \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) \cdot (\mathbf{x}_{t+\Delta t} - \mathbf{x}_t) + \text{const} \end{align}\)</p> \[\log p(\mathbf{x}_t \mid \mathbf{x}_{t+\Delta t}) = -\frac{(\Delta \mathbf{x} - \mu dt)^2}{2\sigma^2 dt} - \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) \cdot \Delta \mathbf{x} + \text{const}\]]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[Intro]]></summary></entry><entry><title type="html">Ito’s Integral</title><link href="https://kasjfksj.github.io/blog/2025/Ito/" rel="alternate" type="text/html" title="Ito’s Integral"/><published>2025-02-08T10:34:47-08:00</published><updated>2025-02-08T10:34:47-08:00</updated><id>https://kasjfksj.github.io/blog/2025/Ito</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2025/Ito/"><![CDATA[<h1 id="itos-integral">Ito’s Integral</h1> <p>Recall the stochastic differential equation (SDE) form:</p> \[dX_t = \mu(X_t, t)\, dt + \sigma(X_t, t)\, dW_t, \quad X_0 = x_0.\] <p>For simplicity, we’ll focus on the driving noise term and replace \(dW_t\) with \(dB_t\), where \(B_t\) is a standard Brownian motion (Wiener process).</p> <p>Integrating both sides gives:</p> \[X_T - X_0 = \int_0^T \mu(X_t, t)\, dt + \int_0^T \sigma(X_t, t)\, dB_t.\] <p>The first integral is a standard Riemann integral—familiar and deterministic. But the second term is trickier: it’s a stochastic integral with respect to the highly irregular paths of Brownian motion. We haven’t yet defined what \(\int f(t) \, dB_t\) even means.</p> <p>This note builds intuition for why stochastic integration works by exploring two key properties of Brownian motion paths: total variation and quadratic variation.</p> <h2 id="total-variation">Total Variation</h2> <p>Consider a partition of the interval \([0, T]\) into \(n\) equal parts: \(t_k = k \cdot \Delta t\), where \(\Delta t = T/n\) and \(k = 0, 1, \dots, n-1\).</p> <p>The <strong>total variation</strong> along this partition is the sum of absolute increments:</p> \[V_n = \sum_{k=0}^{n-1} |\Delta B_{t_k}| = \sum_{k=0}^{n-1} |B_{t_{k+1}} - B_{t_k}|,\] <p>where each \(\Delta B_{t_k} \sim \mathcal{N}(0, \Delta t)\) independently.</p> <p>Why use absolute values? They measure the “total distance traveled” by the path, regardless of direction.</p> <p>Now compute the expectation:</p> <table> <tbody> <tr> <td>Let \(Z \sim \mathcal{N}(0, \Delta t)\). Then $$\mathbb{E}[</td> <td>Z</td> <td>] = \sqrt{\Delta t} \cdot \sqrt{2/\pi}$$.</td> </tr> </tbody> </table> <p>Thus,</p> \[\mathbb{E}[V_n] = n \cdot \sqrt{\Delta t} \cdot \sqrt{2/\pi} = \sqrt{n} \cdot \sqrt{T} \cdot \sqrt{2/\pi} = \sqrt{n T} \cdot \sqrt{2/\pi}.\] <p>As \(n \to \infty\), \(\mathbb{E}[V_n] \to \infty\).</p> <p>This shows that the total variation of a Brownian path over \([0, T]\) is infinite. The path is continuous but has infinite length—like a fractal curve that wiggles infinitely much at every scale. This is why Brownian paths are nowhere differentiable.</p> <h2 id="quadratic-variation">Quadratic Variation</h2> <p>Now consider the sum of squared increments :</p> \[Q_n = \sum_{k=0}^{n-1} (\Delta B_{t_k})^2.\] <p>This is the <strong>quadratic variation</strong> along the partition.</p> <p>Expectation:</p> \[\mathbb{E}[Q_n] = \sum_{k=0}^{n-1} \mathbb{E}[(\Delta B_{t_k})^2] = \sum_{k=0}^{n-1} \Delta t = T.\] <p>Variance:</p> <p>For \(Z \sim \mathcal{N}(0, \Delta t)\), \(Z^2\) has variance \(2 (\Delta t)^2\), so</p> \[\mathrm{Var}(Q_n) = n \cdot 2 (\Delta t)^2 = 2 T^2 / n \to 0 \quad \text{as } n \to \infty.\] <p>Thus, the quadratic variation of Brownian motion over \([0, T]\) is finite and equals 0.</p> <p>Observe that if \(n \to \infty\), \(\mathbb{E}[(\Delta B_t)^2]=\Delta t \\ \mathrm{Var}((\Delta B_t)^2)=0\), we can somehow treat \((\Delta B_t)^2\) as a number instead of random variable, which leads to the formula that \(dB_t \cdot dB_t=dt\)</p>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[Ito’s Integral Recall the stochastic differential equation (SDE) form:]]></summary></entry><entry><title type="html">Grokking - a possible way to achieve AGI</title><link href="https://kasjfksj.github.io/blog/2024/Analysis-Grokfast/" rel="alternate" type="text/html" title="Grokking - a possible way to achieve AGI"/><published>2024-10-10T06:45:52-07:00</published><updated>2024-10-10T06:45:52-07:00</updated><id>https://kasjfksj.github.io/blog/2024/Analysis-Grokfast</id><content type="html" xml:base="https://kasjfksj.github.io/blog/2024/Analysis-Grokfast/"><![CDATA[<p>As mentioned in previous blog, grokking is a phenomenon that extremely long training time leads to a sharp incease in val accuracy. Some researchers try to find a way to speed up the grokking phenomenon. GrokFast proposed a new optimizer to boost slow-varying component of the gradient as they hypothesized that it was a contributing factor to Grokking phenomenon.</p> <p>Initially, I believe that GrokFast could be significant and was puzzled when there was only 1 citation currently. However, further testing on GrokFast shows that it was not capable enough to speed up generalization on test dataset. Default setup, using 3 layer and 200 hidden layer size can achieve good result on fast grokking, but other setup will result in much slower learning procedure, resulting much later increase in train accuracy and val accuracy. The test play results are presented below</p> <h1 id="grokfast---testing">GrokFast - testing</h1> <p>I first test the default setup mentioned above. The result is quiet amazing, as we can see the much earlier increase in val acc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grok_fast_none.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> No GrokFast </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_em.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> With GrokFast </div> <p>Later, I changed some parameters in GrokFast to test its general capability over all kinds of network. However, the result was not satsifying.</p> <h1 id="grokfast---effect-of-network-depth">GrokFast - effect of network depth</h1> <p>First, I changed the number of layers to 4 and 5 and tested the performance. While train acc and val acc increased simultaneously, the number of steps required to significantly increase train acc and val acc rised from \(10^3\) to around \(10^4\). This phenomenon was quite puzzling, and I speculated that it was due to the magnifying low-varying gradient component.</p> <p>However, there’s still some interesting results of the experiment.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_4layer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 4layer Grokfast experienced sharper increase than 3layer Grokfast </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_5layer.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 5layer Grokfast experienced sharper increase than 4layer Grokfast at $$10^4$$ steps </div> <p>We can see that the network experienced sharper increase at \(10^4\) steps.</p> <h1 id="grokfast---effect-of-network-width">GrokFast - effect of network width</h1> <p>Next, I want to test the effect of network width on GrokFast performance, with the setup of 3 layers of network.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/aassets/img/grok_fast/grokfast_128p.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> at around $$10^4$$ steps, the test acc experienced a decrease and then slowly increase at around 5*10^4 steps </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast_512p.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> at around $$10^4$$ steps, the test acc experienced a slight decrease and then rapidly increase. </div> <p>Based on the observation above, I conclude that network width can help network regain its val accuracy after the mysterious drop in val acc.</p> <h1 id="combination-with-lora">Combination with LoRA</h1> <p>Due to the phenomenon, I planed to use Grokfast on LoRA. Specifically, using the gradient update code on matrix A and matrix B can work maybe. However, the first test result was confusing.</p> <p>The first setup is traditional 3 layer + 200 hidden size. GrokFast implemented MA optimizer, and here’s the result</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/grok_fast/grokfast+LoRA.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>While val acc and train acc climbed up together at around 10^4, val acc suddenly dropped and continued to decrease until the end of training time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/aassets/img/grok_fast/grokfast_ema.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For ema optimizer, the val acc sharply increase at \(10^4\) steps and then increases at a very slow pace.</p>]]></content><author><name></name></author><category term="AI"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[As mentioned in previous blog, grokking is a phenomenon that extremely long training time leads to a sharp incease in val accuracy. Some researchers try to find a way to speed up the grokking phenomenon. GrokFast proposed a new optimizer to boost slow-varying component of the gradient as they hypothesized that it was a contributing factor to Grokking phenomenon.]]></summary></entry></feed>